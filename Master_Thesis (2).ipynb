{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh94aQfN2ZS9"
      },
      "outputs": [],
      "source": [
        "#!unzip \"/content/drive/MyDrive/MY WORK/Copy of 50_Speakers.zip\" -d \"/content/drive/MyDrive/50_speaker\""
      ],
      "id": "kh94aQfN2ZS9"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddDRg_fK2oDZ",
        "outputId": "ca565693-6d1c-45ef-a743-58c0b18a9e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "ddDRg_fK2oDZ"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1e9d2a4a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing"
      ],
      "id": "1e9d2a4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c18ba5a"
      },
      "outputs": [],
      "source": [
        "#Dataset link-https://www.kaggle.com/datasets/vjcalling/speaker-recognition-audio-dataset"
      ],
      "id": "8c18ba5a"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7b82cc60"
      },
      "outputs": [],
      "source": [
        "audio_loc=[]\n",
        "labels=[]\n",
        "speakers_folders=os.listdir(\"/content/drive/MyDrive/50_speaker/archive/50_speakers_audio_data\")\n",
        "for folder in speakers_folders:\n",
        "    \n",
        "    audio_files=os.listdir(\"/content/drive/MyDrive/50_speaker/archive/50_speakers_audio_data/\"+folder)\n",
        "    for audio in audio_files:\n",
        "        audio_loc.append(\"/content/drive/MyDrive/50_speaker/archive/50_speakers_audio_data/\"+folder+'/'+audio)\n",
        "        if '_' in folder:\n",
        "            labels.append(folder.split('_')[1])\n",
        "        else:\n",
        "            labels.append(folder[7:])"
      ],
      "id": "7b82cc60"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bc7c60df"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(labels)\n",
        "new_labels=le.transform(labels)\n",
        "new_labels=list(new_labels)"
      ],
      "id": "bc7c60df"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3a5a1e9f"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(list(zip(audio_loc,new_labels)),columns =['File Location','Labels'])"
      ],
      "id": "3a5a1e9f"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "805793f1",
        "outputId": "9aa1486d-49f5-492c-c58c-ee7125ec88bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       File Location  Labels\n",
              "0  /content/drive/MyDrive/50_speaker/archive/50_s...      40\n",
              "1  /content/drive/MyDrive/50_speaker/archive/50_s...      40\n",
              "2  /content/drive/MyDrive/50_speaker/archive/50_s...      40\n",
              "3  /content/drive/MyDrive/50_speaker/archive/50_s...      40\n",
              "4  /content/drive/MyDrive/50_speaker/archive/50_s...      40"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee47b527-ae2d-4ca1-9cd4-ab183e8fcb45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File Location</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/50_speaker/archive/50_s...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/50_speaker/archive/50_s...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/50_speaker/archive/50_s...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/50_speaker/archive/50_s...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/50_speaker/archive/50_s...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee47b527-ae2d-4ca1-9cd4-ab183e8fcb45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee47b527-ae2d-4ca1-9cd4-ab183e8fcb45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee47b527-ae2d-4ca1-9cd4-ab183e8fcb45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data.head()"
      ],
      "id": "805793f1"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3e378d24"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['File Location'],new_labels,stratify=new_labels,test_size=0.20)"
      ],
      "id": "3e378d24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6f6000d"
      },
      "outputs": [],
      "source": [
        "#mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)"
      ],
      "id": "f6f6000d"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d57827f7"
      },
      "outputs": [],
      "source": [
        "audio=data['File Location'][0]"
      ],
      "id": "d57827f7"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f8c4f31",
        "outputId": "511c58ee-f61c-43ab-86a4-9786e8fcd7a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/50_speaker/archive/50_speakers_audio_data/Speaker0041/Speaker0041_004.wav'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "audio"
      ],
      "id": "7f8c4f31"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e5c62a18"
      },
      "outputs": [],
      "source": [
        "signal, sample_rate = librosa.load(audio, sr=22050,duration=2)\n",
        "hop_length=512\n",
        "#sgram_mag, _ = librosa.magphase(signal)"
      ],
      "id": "e5c62a18"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8751b0e2"
      },
      "outputs": [],
      "source": [
        "mel_spectogram=librosa.feature.melspectrogram(y=signal,sr=sample_rate,hop_length=hop_length)"
      ],
      "id": "8751b0e2"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "77d2bc02",
        "outputId": "4d8688c8-d5eb-4ee6-9b5e-9f8ec7ba8567"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "#Transforming the amplitude to decibel\n",
        "S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "img = librosa.display.specshow(S_dB, x_axis='time',\n",
        "                         y_axis='mel', sr=sample_rate,\n",
        "                         fmax=8000, ax=ax)\n",
        "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
        "ax.set(title='Mel-frequency spectrogram')\n",
        "fig.savefig('spectrogram.jpg', dpi=300, bbox_inches='tight')"
      ],
      "id": "77d2bc02"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "357ec02a",
        "outputId": "34d42de3-e516-4f2b-accc-28fd4e721ff7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 87)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "S_dB.shape"
      ],
      "id": "357ec02a"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "38fc3182"
      },
      "outputs": [],
      "source": [
        "# Get mel spectrograms\n",
        "\n",
        "def mel_spectrogram_gen(audio):\n",
        "    signal, sample_rate = librosa.load(audio, sr=22050,duration=3)\n",
        "    hop_length=512\n",
        "    mel_spectogram=librosa.feature.melspectrogram(signal,sample_rate,hop_length=hop_length)\n",
        "    S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "    \n",
        "    return S_dB\n",
        "\n"
      ],
      "id": "38fc3182"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f48449ba"
      },
      "outputs": [],
      "source": [
        "#Getting mel spectograms for all the audio clips in the train data\n",
        "\n",
        "X_train_mel=[]\n",
        "\n",
        "hop_length=512\n",
        "\n",
        "for audio in X_train:\n",
        "    signal, sample_rate = librosa.load(audio, sr=22050,duration=3)\n",
        "    hop_length=512\n",
        "    mel_spectogram=librosa.feature.melspectrogram(y=signal,sr=sample_rate,hop_length=hop_length)\n",
        "    S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "    X_train_mel.append(S_dB)\n",
        "    \n",
        "    "
      ],
      "id": "f48449ba"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "830f0663"
      },
      "outputs": [],
      "source": [
        "rows=[]\n",
        "columns=[]\n",
        "\n",
        "for mel in X_train_mel:\n",
        "    rows.append(mel.shape[0])\n",
        "    columns.append(mel.shape[1])"
      ],
      "id": "830f0663"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "e3b2fe7c"
      },
      "outputs": [],
      "source": [
        "#Getting max pad length\n",
        "\n",
        "max_pad_len=np.max(columns)"
      ],
      "id": "e3b2fe7c"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a4e1826",
        "outputId": "dae604c6-cada-42b7-87e6-13df14e78673"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "max_pad_len"
      ],
      "id": "7a4e1826"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "93ee83ea"
      },
      "outputs": [],
      "source": [
        "#Padding all the mel spectograms\n",
        "X_train_mel_padded=[]\n",
        "\n",
        "for mel in X_train_mel:\n",
        "    padded_mel=np.pad(mel,((0,0),(0,max_pad_len-mel.shape[1])),'constant',constant_values=0)\n",
        "    X_train_mel_padded.append(padded_mel)\n",
        "    \n",
        "X_train_mel_padded=np.array(X_train_mel_padded)\n",
        "X_train_mel_padded=X_train_mel_padded/255"
      ],
      "id": "93ee83ea"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1dac3768"
      },
      "outputs": [],
      "source": [
        "#Getting mel spectograms for all the audio clips in the test data\n",
        "\n",
        "X_test_mel=[]\n",
        "\n",
        "\n",
        "for audio in X_test:\n",
        "    signal, sample_rate = librosa.load(audio, sr=22050,duration=3)\n",
        "    hop_length=512\n",
        "    mel_spectogram=librosa.feature.melspectrogram(y=signal,sr=sample_rate,hop_length=hop_length)\n",
        "    S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "    X_test_mel.append(S_dB)"
      ],
      "id": "1dac3768"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "13f97e59"
      },
      "outputs": [],
      "source": [
        "#Padding all the mel spectrograms in test data\n",
        "\n",
        "X_test_mel_padded=[]\n",
        "\n",
        "for mel in X_test_mel:\n",
        "    if mel.shape[1]<max_pad_len:\n",
        "        padded_mel=np.pad(mel,((0,0),(0,max_pad_len-mel.shape[1])),'constant',constant_values=0)\n",
        "    else:\n",
        "        padded_mel=mel[:,:max_pad_len]\n",
        "    X_test_mel_padded.append(padded_mel)\n",
        "    \n",
        "X_test_mel_padded=np.array(X_test_mel_padded)\n",
        "X_test_mel_padded=X_test_mel_padded/255"
      ],
      "id": "13f97e59"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2e81ddcc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# making positive & negative image pairs\n",
        "\n",
        "def make_pairs(images, labels):\n",
        "      \n",
        "      labels=np.array(labels)\n",
        "      # initialize two empty lists to hold the (image, image) pairs and\n",
        "      # labels to indicate if a pair is positive or negative\n",
        "      pairImages = []\n",
        "      pairLabels = []\n",
        "\n",
        "      numClasses = len(np.unique(labels)) # find all unique class labels\n",
        "     \n",
        "      # idxs -> list of all indexes that belong to the current class label i\n",
        "      idx = [np.where(labels == i)[0] for i in range(0, numClasses)]\n",
        "     \n",
        "      # loop over all images\n",
        "      for idxA in range(len(images)):\n",
        "        # grab the current image and label belonging to the current\n",
        "        # iteration\n",
        "        currentImage = images[idxA]\n",
        "        label =labels[idxA]\n",
        "        # randomly pick an image that belongs to the *same* class\n",
        "        # label\n",
        "        idxB = np.random.choice(idx[label])\n",
        "        posImage = images[idxB]\n",
        "        # prepare a positive pair and update the images and labels\n",
        "        # lists, respectively\n",
        "        pairImages.append([currentImage, posImage])\n",
        "        pairLabels.append([1])\n",
        "        # grab the indices for each of the class labels *not* equal to\n",
        "        # the current label i.e., idx of negative pair image\n",
        "        negIdx = np.where(labels != label)[0]\n",
        "        # images corresponding to negIdx\n",
        "        negImage = images[np.random.choice(negIdx)]\n",
        "        # prepare a negative pair of images and update our lists\n",
        "        pairImages.append([currentImage, negImage])\n",
        "        pairLabels.append([0])\n",
        "        # return a 2-tuple of our image pairs and labels\n",
        "        # i.e., \n",
        "      return (np.array(pairImages), np.array(pairLabels))"
      ],
      "id": "2e81ddcc"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "c9e6e832"
      },
      "outputs": [],
      "source": [
        "# add a channel dimension to the images\n",
        "\n",
        "X_train_mel_padded = np.expand_dims(X_train_mel_padded, axis=-1)\n",
        "X_test_mel_padded = np.expand_dims(X_test_mel_padded, axis=-1)"
      ],
      "id": "c9e6e832"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8c2a92a3"
      },
      "outputs": [],
      "source": [
        "#Making Data Pair for Train Data\n",
        "Pair_train,Label_train=make_pairs(X_train_mel_padded,y_train)"
      ],
      "id": "8c2a92a3"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "93de8fd4"
      },
      "outputs": [],
      "source": [
        "#Making Data Pair for Test Data\n",
        "\n",
        "Pair_test,Label_test=make_pairs(X_test_mel_padded,y_test)"
      ],
      "id": "93de8fd4"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f67422a",
        "outputId": "777515e7-f727-4936-d95a-d94368d7f195"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4016, 2, 128, 130, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "Pair_train.shape"
      ],
      "id": "5f67422a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b1925ad"
      },
      "source": [
        "# Building Siamese Network"
      ],
      "id": "2b1925ad"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "26286a52"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Lambda\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "id": "26286a52"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "15fe4124"
      },
      "outputs": [],
      "source": [
        "def build_siamese_model(inputShape, embeddingDim=48):\n",
        "  # specify the inputs for the feature extractor network\n",
        "  inputs = Input(inputShape)\n",
        "  # define the first set of CONV => RELU => POOL => DROPOUT layers\n",
        "  x = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\",kernel_initializer=tf.keras.initializers.he_normal(seed=0))(inputs)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  # second set of CONV => RELU => POOL => DROPOUT layers\n",
        "  x = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\",kernel_initializer=tf.keras.initializers.he_normal(seed=0))(x)\n",
        "  x = MaxPooling2D(pool_size=2)(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  BatchNorm = tf.keras.layers.BatchNormalization()(x)\n",
        "  # prepare the final outputs\n",
        "  pooledOutput = GlobalAveragePooling2D()(BatchNorm)\n",
        "  outputs = Dense(embeddingDim)(pooledOutput) # output dense layer\n",
        "  # build the model\n",
        "  model = Model(inputs, outputs)\n",
        "  # return the model to the calling function\n",
        "  return model"
      ],
      "id": "15fe4124"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2c3e0de6"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(vectors):\n",
        "\t# unpack the vectors into separate lists\n",
        "\t(featsA, featsB) = vectors\n",
        "\t# compute the sum of squared distances between the vectors\n",
        "\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
        "\t\tkeepdims=True)\n",
        "\t# return the euclidean distance between the vectors\n",
        "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))"
      ],
      "id": "2c3e0de6"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "f8cc4927"
      },
      "outputs": [],
      "source": [
        "max_pad_len=130"
      ],
      "id": "f8cc4927"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5f8edb7c"
      },
      "outputs": [],
      "source": [
        "# specify the shape of the inputs for our network\n",
        "IMG_SHAPE = (128,max_pad_len, 1)\n",
        "# specify the batch size and number of epochs\n",
        "BATCH_SIZE =64\n",
        "EPOCHS =500"
      ],
      "id": "5f8edb7c"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8ee9d5da"
      },
      "outputs": [],
      "source": [
        "architecture = build_siamese_model(inputShape=IMG_SHAPE, embeddingDim=48)"
      ],
      "id": "8ee9d5da"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d9e2898",
        "outputId": "e607cecc-8776-4852-b8d1-89fdf224adb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 130, 1)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 128, 130, 64)      320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 64, 65, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64, 65, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 64, 65, 64)        16448     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 32, 32, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 64)               0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 48)                3120      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,144\n",
            "Trainable params: 20,016\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "architecture.summary()"
      ],
      "id": "2d9e2898"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "08d3b498"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "import random as rn\n",
        "## Set the random seed values to regenerate the model.\n",
        "np.random.seed(0)\n",
        "rn.seed(0)\n",
        "\n",
        "imgA = Input(shape=IMG_SHAPE)\n",
        "imgB = Input(shape=IMG_SHAPE)\n",
        "featureExtractor = build_siamese_model(IMG_SHAPE)\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)"
      ],
      "id": "08d3b498"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "e237f6ad"
      },
      "outputs": [],
      "source": [
        "# finally, construct the siamese network\n",
        "distance = Lambda(euclidean_distance)([featsA, featsB])\n",
        "outputs = Dense(1, activation=\"sigmoid\")(distance)\n",
        "model = Model(inputs=[imgA, imgB], outputs=outputs)"
      ],
      "id": "e237f6ad"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "09efbb2c"
      },
      "outputs": [],
      "source": [
        "# compiling the model\n",
        "#opt=tf.keras.optimizers.SGD(learning_rate=0.1,momentum=0.9)\n",
        "#opt =tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
      ],
      "id": "09efbb2c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8651f3bd"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='max') \n",
        "\n",
        "#LrOnPlateau scheduler \n",
        "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.1,patience=5,min_lr=0.001)\n"
      ],
      "id": "8651f3bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ac183c6",
        "outputId": "29dd74ce-d9af-4afb-ba5a-3e470e91a2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.5000\n",
            "Epoch 1: val_accuracy improved from -inf to 0.61730, saving model to model_save/weights-01-0.6173.hdf5\n",
            "63/63 [==============================] - 27s 140ms/step - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6886 - val_accuracy: 0.6173\n",
            "Epoch 2/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.5000\n",
            "Epoch 2: val_accuracy improved from 0.61730 to 0.65308, saving model to model_save/weights-02-0.6531.hdf5\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.6867 - accuracy: 0.5000 - val_loss: 0.6826 - val_accuracy: 0.6531\n",
            "Epoch 3/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.5027\n",
            "Epoch 3: val_accuracy improved from 0.65308 to 0.69284, saving model to model_save/weights-03-0.6928.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.6766 - accuracy: 0.5027 - val_loss: 0.6789 - val_accuracy: 0.6928\n",
            "Epoch 4/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6643 - accuracy: 0.5438\n",
            "Epoch 4: val_accuracy did not improve from 0.69284\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.6643 - accuracy: 0.5438 - val_loss: 0.6588 - val_accuracy: 0.6759\n",
            "Epoch 5/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6514 - accuracy: 0.5954\n",
            "Epoch 5: val_accuracy improved from 0.69284 to 0.70676, saving model to model_save/weights-05-0.7068.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.6514 - accuracy: 0.5954 - val_loss: 0.6479 - val_accuracy: 0.7068\n",
            "Epoch 6/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6397 - accuracy: 0.6387\n",
            "Epoch 6: val_accuracy improved from 0.70676 to 0.72962, saving model to model_save/weights-06-0.7296.hdf5\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.6397 - accuracy: 0.6387 - val_loss: 0.6326 - val_accuracy: 0.7296\n",
            "Epoch 7/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.6614\n",
            "Epoch 7: val_accuracy did not improve from 0.72962\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.6279 - accuracy: 0.6614 - val_loss: 0.6196 - val_accuracy: 0.7266\n",
            "Epoch 8/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6159 - accuracy: 0.6713\n",
            "Epoch 8: val_accuracy did not improve from 0.72962\n",
            "63/63 [==============================] - 8s 123ms/step - loss: 0.6159 - accuracy: 0.6713 - val_loss: 0.6100 - val_accuracy: 0.7237\n",
            "Epoch 9/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.6887\n",
            "Epoch 9: val_accuracy did not improve from 0.72962\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.6066 - accuracy: 0.6887 - val_loss: 0.6079 - val_accuracy: 0.7177\n",
            "Epoch 10/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.6965\n",
            "Epoch 10: val_accuracy did not improve from 0.72962\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.5975 - accuracy: 0.6965 - val_loss: 0.5978 - val_accuracy: 0.7266\n",
            "Epoch 11/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5860 - accuracy: 0.7126\n",
            "Epoch 11: val_accuracy improved from 0.72962 to 0.73559, saving model to model_save/weights-11-0.7356.hdf5\n",
            "63/63 [==============================] - 8s 119ms/step - loss: 0.5860 - accuracy: 0.7126 - val_loss: 0.5761 - val_accuracy: 0.7356\n",
            "Epoch 12/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.7251\n",
            "Epoch 12: val_accuracy improved from 0.73559 to 0.79920, saving model to model_save/weights-12-0.7992.hdf5\n",
            "63/63 [==============================] - 8s 120ms/step - loss: 0.5755 - accuracy: 0.7251 - val_loss: 0.5602 - val_accuracy: 0.7992\n",
            "Epoch 13/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.7288\n",
            "Epoch 13: val_accuracy improved from 0.79920 to 0.80616, saving model to model_save/weights-13-0.8062.hdf5\n",
            "63/63 [==============================] - 8s 122ms/step - loss: 0.5623 - accuracy: 0.7288 - val_loss: 0.5505 - val_accuracy: 0.8062\n",
            "Epoch 14/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.7475\n",
            "Epoch 14: val_accuracy did not improve from 0.80616\n",
            "63/63 [==============================] - 8s 121ms/step - loss: 0.5563 - accuracy: 0.7475 - val_loss: 0.5574 - val_accuracy: 0.7445\n",
            "Epoch 15/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.7550\n",
            "Epoch 15: val_accuracy did not improve from 0.80616\n",
            "63/63 [==============================] - 8s 121ms/step - loss: 0.5446 - accuracy: 0.7550 - val_loss: 0.5276 - val_accuracy: 0.7793\n",
            "Epoch 16/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.7647\n",
            "Epoch 16: val_accuracy did not improve from 0.80616\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.5372 - accuracy: 0.7647 - val_loss: 0.5151 - val_accuracy: 0.7863\n",
            "Epoch 17/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.7717\n",
            "Epoch 17: val_accuracy did not improve from 0.80616\n",
            "63/63 [==============================] - 8s 119ms/step - loss: 0.5251 - accuracy: 0.7717 - val_loss: 0.5235 - val_accuracy: 0.7714\n",
            "Epoch 18/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.7849\n",
            "Epoch 18: val_accuracy improved from 0.80616 to 0.81312, saving model to model_save/weights-18-0.8131.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.5174 - accuracy: 0.7849 - val_loss: 0.4986 - val_accuracy: 0.8131\n",
            "Epoch 19/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5090 - accuracy: 0.7834\n",
            "Epoch 19: val_accuracy improved from 0.81312 to 0.81809, saving model to model_save/weights-19-0.8181.hdf5\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.5090 - accuracy: 0.7834 - val_loss: 0.4829 - val_accuracy: 0.8181\n",
            "Epoch 20/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5011 - accuracy: 0.7908\n",
            "Epoch 20: val_accuracy did not improve from 0.81809\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.5011 - accuracy: 0.7908 - val_loss: 0.4863 - val_accuracy: 0.8151\n",
            "Epoch 21/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4893 - accuracy: 0.8030\n",
            "Epoch 21: val_accuracy improved from 0.81809 to 0.83002, saving model to model_save/weights-21-0.8300.hdf5\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.4893 - accuracy: 0.8030 - val_loss: 0.4647 - val_accuracy: 0.8300\n",
            "Epoch 22/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.8060\n",
            "Epoch 22: val_accuracy improved from 0.83002 to 0.83797, saving model to model_save/weights-22-0.8380.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.4795 - accuracy: 0.8060 - val_loss: 0.4581 - val_accuracy: 0.8380\n",
            "Epoch 23/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4671 - accuracy: 0.8150\n",
            "Epoch 23: val_accuracy did not improve from 0.83797\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.4671 - accuracy: 0.8150 - val_loss: 0.4554 - val_accuracy: 0.8350\n",
            "Epoch 24/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4577 - accuracy: 0.8225\n",
            "Epoch 24: val_accuracy did not improve from 0.83797\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.4577 - accuracy: 0.8225 - val_loss: 0.4553 - val_accuracy: 0.8161\n",
            "Epoch 25/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4459 - accuracy: 0.8267\n",
            "Epoch 25: val_accuracy did not improve from 0.83797\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.4459 - accuracy: 0.8267 - val_loss: 0.4530 - val_accuracy: 0.8231\n",
            "Epoch 26/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.8277\n",
            "Epoch 26: val_accuracy improved from 0.83797 to 0.83996, saving model to model_save/weights-26-0.8400.hdf5\n",
            "63/63 [==============================] - 8s 119ms/step - loss: 0.4453 - accuracy: 0.8277 - val_loss: 0.4307 - val_accuracy: 0.8400\n",
            "Epoch 27/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.8312\n",
            "Epoch 27: val_accuracy improved from 0.83996 to 0.84791, saving model to model_save/weights-27-0.8479.hdf5\n",
            "63/63 [==============================] - 8s 120ms/step - loss: 0.4404 - accuracy: 0.8312 - val_loss: 0.4251 - val_accuracy: 0.8479\n",
            "Epoch 28/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.8307\n",
            "Epoch 28: val_accuracy did not improve from 0.84791\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.4339 - accuracy: 0.8307 - val_loss: 0.4150 - val_accuracy: 0.8469\n",
            "Epoch 29/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.8376\n",
            "Epoch 29: val_accuracy did not improve from 0.84791\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.4247 - accuracy: 0.8376 - val_loss: 0.4096 - val_accuracy: 0.8449\n",
            "Epoch 30/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8384\n",
            "Epoch 30: val_accuracy did not improve from 0.84791\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.4183 - accuracy: 0.8384 - val_loss: 0.4074 - val_accuracy: 0.8429\n",
            "Epoch 31/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4122 - accuracy: 0.8416\n",
            "Epoch 31: val_accuracy improved from 0.84791 to 0.85686, saving model to model_save/weights-31-0.8569.hdf5\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.4122 - accuracy: 0.8416 - val_loss: 0.3972 - val_accuracy: 0.8569\n",
            "Epoch 32/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.8404\n",
            "Epoch 32: val_accuracy did not improve from 0.85686\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.4085 - accuracy: 0.8404 - val_loss: 0.3976 - val_accuracy: 0.8439\n",
            "Epoch 33/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4047 - accuracy: 0.8444\n",
            "Epoch 33: val_accuracy did not improve from 0.85686\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.4047 - accuracy: 0.8444 - val_loss: 0.3943 - val_accuracy: 0.8489\n",
            "Epoch 34/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8471\n",
            "Epoch 34: val_accuracy did not improve from 0.85686\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.4003 - accuracy: 0.8471 - val_loss: 0.3916 - val_accuracy: 0.8509\n",
            "Epoch 35/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.8533\n",
            "Epoch 35: val_accuracy did not improve from 0.85686\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.3921 - accuracy: 0.8533 - val_loss: 0.3890 - val_accuracy: 0.8419\n",
            "Epoch 36/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3911 - accuracy: 0.8499\n",
            "Epoch 36: val_accuracy did not improve from 0.85686\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3911 - accuracy: 0.8499 - val_loss: 0.3839 - val_accuracy: 0.8459\n",
            "Epoch 37/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3866 - accuracy: 0.8533\n",
            "Epoch 37: val_accuracy did not improve from 0.85686\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3866 - accuracy: 0.8533 - val_loss: 0.3850 - val_accuracy: 0.8569\n",
            "Epoch 38/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3816 - accuracy: 0.8568\n",
            "Epoch 38: val_accuracy improved from 0.85686 to 0.86481, saving model to model_save/weights-38-0.8648.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.3816 - accuracy: 0.8568 - val_loss: 0.3728 - val_accuracy: 0.8648\n",
            "Epoch 39/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8586\n",
            "Epoch 39: val_accuracy did not improve from 0.86481\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3791 - accuracy: 0.8586 - val_loss: 0.3784 - val_accuracy: 0.8579\n",
            "Epoch 40/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8553\n",
            "Epoch 40: val_accuracy did not improve from 0.86481\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3749 - accuracy: 0.8553 - val_loss: 0.3918 - val_accuracy: 0.8370\n",
            "Epoch 41/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3734 - accuracy: 0.8588\n",
            "Epoch 41: val_accuracy improved from 0.86481 to 0.86779, saving model to model_save/weights-41-0.8678.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3734 - accuracy: 0.8588 - val_loss: 0.3644 - val_accuracy: 0.8678\n",
            "Epoch 42/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3705 - accuracy: 0.8596\n",
            "Epoch 42: val_accuracy did not improve from 0.86779\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.3705 - accuracy: 0.8596 - val_loss: 0.3721 - val_accuracy: 0.8519\n",
            "Epoch 43/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.8608\n",
            "Epoch 43: val_accuracy did not improve from 0.86779\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3653 - accuracy: 0.8608 - val_loss: 0.3645 - val_accuracy: 0.8469\n",
            "Epoch 44/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3650 - accuracy: 0.8640\n",
            "Epoch 44: val_accuracy improved from 0.86779 to 0.87376, saving model to model_save/weights-44-0.8738.hdf5\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3650 - accuracy: 0.8640 - val_loss: 0.3509 - val_accuracy: 0.8738\n",
            "Epoch 45/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.8668\n",
            "Epoch 45: val_accuracy did not improve from 0.87376\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3563 - accuracy: 0.8668 - val_loss: 0.3747 - val_accuracy: 0.8519\n",
            "Epoch 46/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.8633\n",
            "Epoch 46: val_accuracy did not improve from 0.87376\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.3632 - accuracy: 0.8633 - val_loss: 0.3515 - val_accuracy: 0.8628\n",
            "Epoch 47/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.8683\n",
            "Epoch 47: val_accuracy did not improve from 0.87376\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3565 - accuracy: 0.8683 - val_loss: 0.3545 - val_accuracy: 0.8519\n",
            "Epoch 48/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.8690\n",
            "Epoch 48: val_accuracy did not improve from 0.87376\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3496 - accuracy: 0.8690 - val_loss: 0.3391 - val_accuracy: 0.8618\n",
            "Epoch 49/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8688\n",
            "Epoch 49: val_accuracy improved from 0.87376 to 0.87575, saving model to model_save/weights-49-0.8757.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3454 - accuracy: 0.8688 - val_loss: 0.3381 - val_accuracy: 0.8757\n",
            "Epoch 50/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8713\n",
            "Epoch 50: val_accuracy did not improve from 0.87575\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.3420 - accuracy: 0.8713 - val_loss: 0.3371 - val_accuracy: 0.8688\n",
            "Epoch 51/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3337 - accuracy: 0.8827\n",
            "Epoch 51: val_accuracy improved from 0.87575 to 0.87873, saving model to model_save/weights-51-0.8787.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3337 - accuracy: 0.8827 - val_loss: 0.3325 - val_accuracy: 0.8787\n",
            "Epoch 52/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3366 - accuracy: 0.8792\n",
            "Epoch 52: val_accuracy did not improve from 0.87873\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3366 - accuracy: 0.8792 - val_loss: 0.3349 - val_accuracy: 0.8738\n",
            "Epoch 53/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.8807\n",
            "Epoch 53: val_accuracy improved from 0.87873 to 0.88469, saving model to model_save/weights-53-0.8847.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3340 - accuracy: 0.8807 - val_loss: 0.3307 - val_accuracy: 0.8847\n",
            "Epoch 54/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3275 - accuracy: 0.8850\n",
            "Epoch 54: val_accuracy did not improve from 0.88469\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3275 - accuracy: 0.8850 - val_loss: 0.3167 - val_accuracy: 0.8817\n",
            "Epoch 55/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.8917\n",
            "Epoch 55: val_accuracy improved from 0.88469 to 0.88867, saving model to model_save/weights-55-0.8887.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3228 - accuracy: 0.8917 - val_loss: 0.3115 - val_accuracy: 0.8887\n",
            "Epoch 56/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.8867\n",
            "Epoch 56: val_accuracy improved from 0.88867 to 0.89066, saving model to model_save/weights-56-0.8907.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.3223 - accuracy: 0.8867 - val_loss: 0.3098 - val_accuracy: 0.8907\n",
            "Epoch 57/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8902\n",
            "Epoch 57: val_accuracy did not improve from 0.89066\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.3132 - accuracy: 0.8902 - val_loss: 0.3255 - val_accuracy: 0.8748\n",
            "Epoch 58/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.8932\n",
            "Epoch 58: val_accuracy did not improve from 0.89066\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3129 - accuracy: 0.8932 - val_loss: 0.3215 - val_accuracy: 0.8767\n",
            "Epoch 59/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.8959\n",
            "Epoch 59: val_accuracy improved from 0.89066 to 0.89662, saving model to model_save/weights-59-0.8966.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.3040 - accuracy: 0.8959 - val_loss: 0.3033 - val_accuracy: 0.8966\n",
            "Epoch 60/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.8969\n",
            "Epoch 60: val_accuracy improved from 0.89662 to 0.89960, saving model to model_save/weights-60-0.8996.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.3035 - accuracy: 0.8969 - val_loss: 0.2934 - val_accuracy: 0.8996\n",
            "Epoch 61/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8944\n",
            "Epoch 61: val_accuracy did not improve from 0.89960\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.3041 - accuracy: 0.8944 - val_loss: 0.2962 - val_accuracy: 0.8966\n",
            "Epoch 62/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.8952\n",
            "Epoch 62: val_accuracy did not improve from 0.89960\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.3014 - accuracy: 0.8952 - val_loss: 0.2950 - val_accuracy: 0.8877\n",
            "Epoch 63/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.8989\n",
            "Epoch 63: val_accuracy did not improve from 0.89960\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2984 - accuracy: 0.8989 - val_loss: 0.3089 - val_accuracy: 0.8837\n",
            "Epoch 64/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2965 - accuracy: 0.8972\n",
            "Epoch 64: val_accuracy improved from 0.89960 to 0.90159, saving model to model_save/weights-64-0.9016.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2965 - accuracy: 0.8972 - val_loss: 0.2901 - val_accuracy: 0.9016\n",
            "Epoch 65/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9021\n",
            "Epoch 65: val_accuracy did not improve from 0.90159\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2905 - accuracy: 0.9021 - val_loss: 0.2863 - val_accuracy: 0.8936\n",
            "Epoch 66/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.9026\n",
            "Epoch 66: val_accuracy did not improve from 0.90159\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2925 - accuracy: 0.9026 - val_loss: 0.2789 - val_accuracy: 0.8996\n",
            "Epoch 67/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.8974\n",
            "Epoch 67: val_accuracy did not improve from 0.90159\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2903 - accuracy: 0.8974 - val_loss: 0.2877 - val_accuracy: 0.8996\n",
            "Epoch 68/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9024\n",
            "Epoch 68: val_accuracy did not improve from 0.90159\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2862 - accuracy: 0.9024 - val_loss: 0.3036 - val_accuracy: 0.8787\n",
            "Epoch 69/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.9051\n",
            "Epoch 69: val_accuracy did not improve from 0.90159\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2813 - accuracy: 0.9051 - val_loss: 0.2923 - val_accuracy: 0.8907\n",
            "Epoch 70/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.9064\n",
            "Epoch 70: val_accuracy did not improve from 0.90159\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2803 - accuracy: 0.9064 - val_loss: 0.2752 - val_accuracy: 0.9006\n",
            "Epoch 71/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.9076\n",
            "Epoch 71: val_accuracy improved from 0.90159 to 0.90557, saving model to model_save/weights-71-0.9056.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2782 - accuracy: 0.9076 - val_loss: 0.2709 - val_accuracy: 0.9056\n",
            "Epoch 72/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9009\n",
            "Epoch 72: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2751 - accuracy: 0.9009 - val_loss: 0.2714 - val_accuracy: 0.9016\n",
            "Epoch 73/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9091\n",
            "Epoch 73: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2692 - accuracy: 0.9091 - val_loss: 0.2796 - val_accuracy: 0.8976\n",
            "Epoch 74/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9141\n",
            "Epoch 74: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2681 - accuracy: 0.9141 - val_loss: 0.2689 - val_accuracy: 0.9006\n",
            "Epoch 75/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2725 - accuracy: 0.9064\n",
            "Epoch 75: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2725 - accuracy: 0.9064 - val_loss: 0.2792 - val_accuracy: 0.8996\n",
            "Epoch 76/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9109\n",
            "Epoch 76: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2666 - accuracy: 0.9109 - val_loss: 0.2758 - val_accuracy: 0.8996\n",
            "Epoch 77/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9141\n",
            "Epoch 77: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2615 - accuracy: 0.9141 - val_loss: 0.2674 - val_accuracy: 0.8996\n",
            "Epoch 78/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.9153\n",
            "Epoch 78: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2530 - accuracy: 0.9153 - val_loss: 0.2694 - val_accuracy: 0.9016\n",
            "Epoch 79/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9153\n",
            "Epoch 79: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2575 - accuracy: 0.9153 - val_loss: 0.2741 - val_accuracy: 0.9026\n",
            "Epoch 80/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.9128\n",
            "Epoch 80: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2601 - accuracy: 0.9128 - val_loss: 0.2645 - val_accuracy: 0.9026\n",
            "Epoch 81/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.9146\n",
            "Epoch 81: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2520 - accuracy: 0.9146 - val_loss: 0.2619 - val_accuracy: 0.9036\n",
            "Epoch 82/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9203\n",
            "Epoch 82: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2497 - accuracy: 0.9203 - val_loss: 0.2637 - val_accuracy: 0.8956\n",
            "Epoch 83/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9163\n",
            "Epoch 83: val_accuracy did not improve from 0.90557\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2511 - accuracy: 0.9163 - val_loss: 0.2572 - val_accuracy: 0.9026\n",
            "Epoch 84/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.9188\n",
            "Epoch 84: val_accuracy improved from 0.90557 to 0.90855, saving model to model_save/weights-84-0.9085.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2500 - accuracy: 0.9188 - val_loss: 0.2519 - val_accuracy: 0.9085\n",
            "Epoch 85/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.9226\n",
            "Epoch 85: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2399 - accuracy: 0.9226 - val_loss: 0.2539 - val_accuracy: 0.9016\n",
            "Epoch 86/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.9176\n",
            "Epoch 86: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.2423 - accuracy: 0.9176 - val_loss: 0.2547 - val_accuracy: 0.9046\n",
            "Epoch 87/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9203\n",
            "Epoch 87: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2415 - accuracy: 0.9203 - val_loss: 0.2553 - val_accuracy: 0.9036\n",
            "Epoch 88/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9226\n",
            "Epoch 88: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2406 - accuracy: 0.9226 - val_loss: 0.2546 - val_accuracy: 0.9076\n",
            "Epoch 89/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.9193\n",
            "Epoch 89: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2373 - accuracy: 0.9193 - val_loss: 0.2571 - val_accuracy: 0.8936\n",
            "Epoch 90/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9196\n",
            "Epoch 90: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2384 - accuracy: 0.9196 - val_loss: 0.2564 - val_accuracy: 0.9026\n",
            "Epoch 91/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2380 - accuracy: 0.9211\n",
            "Epoch 91: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2380 - accuracy: 0.9211 - val_loss: 0.2488 - val_accuracy: 0.9006\n",
            "Epoch 92/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.9211\n",
            "Epoch 92: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2334 - accuracy: 0.9211 - val_loss: 0.2424 - val_accuracy: 0.9085\n",
            "Epoch 93/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.9223\n",
            "Epoch 93: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2328 - accuracy: 0.9223 - val_loss: 0.2506 - val_accuracy: 0.8976\n",
            "Epoch 94/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9231\n",
            "Epoch 94: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2299 - accuracy: 0.9231 - val_loss: 0.2742 - val_accuracy: 0.9085\n",
            "Epoch 95/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.9153\n",
            "Epoch 95: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2373 - accuracy: 0.9153 - val_loss: 0.2538 - val_accuracy: 0.9066\n",
            "Epoch 96/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9236\n",
            "Epoch 96: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2227 - accuracy: 0.9236 - val_loss: 0.2450 - val_accuracy: 0.8976\n",
            "Epoch 97/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9278\n",
            "Epoch 97: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2236 - accuracy: 0.9278 - val_loss: 0.2388 - val_accuracy: 0.9056\n",
            "Epoch 98/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9226\n",
            "Epoch 98: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2259 - accuracy: 0.9226 - val_loss: 0.2709 - val_accuracy: 0.8946\n",
            "Epoch 99/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.9213\n",
            "Epoch 99: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2247 - accuracy: 0.9213 - val_loss: 0.2303 - val_accuracy: 0.9076\n",
            "Epoch 100/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9253\n",
            "Epoch 100: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2204 - accuracy: 0.9253 - val_loss: 0.2485 - val_accuracy: 0.9046\n",
            "Epoch 101/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9288\n",
            "Epoch 101: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2186 - accuracy: 0.9288 - val_loss: 0.2509 - val_accuracy: 0.9036\n",
            "Epoch 102/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9255\n",
            "Epoch 102: val_accuracy did not improve from 0.90855\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2190 - accuracy: 0.9255 - val_loss: 0.2397 - val_accuracy: 0.9085\n",
            "Epoch 103/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.9280\n",
            "Epoch 103: val_accuracy improved from 0.90855 to 0.91849, saving model to model_save/weights-103-0.9185.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2142 - accuracy: 0.9280 - val_loss: 0.2286 - val_accuracy: 0.9185\n",
            "Epoch 104/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9293\n",
            "Epoch 104: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2130 - accuracy: 0.9293 - val_loss: 0.2500 - val_accuracy: 0.9036\n",
            "Epoch 105/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9263\n",
            "Epoch 105: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2202 - accuracy: 0.9263 - val_loss: 0.2372 - val_accuracy: 0.9016\n",
            "Epoch 106/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.9295\n",
            "Epoch 106: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2172 - accuracy: 0.9295 - val_loss: 0.2324 - val_accuracy: 0.9095\n",
            "Epoch 107/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9320\n",
            "Epoch 107: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2063 - accuracy: 0.9320 - val_loss: 0.2449 - val_accuracy: 0.9046\n",
            "Epoch 108/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2096 - accuracy: 0.9320\n",
            "Epoch 108: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2096 - accuracy: 0.9320 - val_loss: 0.2342 - val_accuracy: 0.9155\n",
            "Epoch 109/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9303\n",
            "Epoch 109: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2090 - accuracy: 0.9303 - val_loss: 0.2339 - val_accuracy: 0.9105\n",
            "Epoch 110/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9348\n",
            "Epoch 110: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2028 - accuracy: 0.9348 - val_loss: 0.2363 - val_accuracy: 0.9165\n",
            "Epoch 111/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.9313\n",
            "Epoch 111: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2056 - accuracy: 0.9313 - val_loss: 0.2256 - val_accuracy: 0.9095\n",
            "Epoch 112/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2026 - accuracy: 0.9343\n",
            "Epoch 112: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2026 - accuracy: 0.9343 - val_loss: 0.2378 - val_accuracy: 0.9125\n",
            "Epoch 113/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9375\n",
            "Epoch 113: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.2024 - accuracy: 0.9375 - val_loss: 0.2265 - val_accuracy: 0.9066\n",
            "Epoch 114/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9313\n",
            "Epoch 114: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2061 - accuracy: 0.9313 - val_loss: 0.2300 - val_accuracy: 0.9135\n",
            "Epoch 115/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.9295\n",
            "Epoch 115: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.2099 - accuracy: 0.9295 - val_loss: 0.2293 - val_accuracy: 0.9076\n",
            "Epoch 116/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9318\n",
            "Epoch 116: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.2005 - accuracy: 0.9318 - val_loss: 0.2315 - val_accuracy: 0.9056\n",
            "Epoch 117/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9330\n",
            "Epoch 117: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1977 - accuracy: 0.9330 - val_loss: 0.2323 - val_accuracy: 0.9066\n",
            "Epoch 118/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9330\n",
            "Epoch 118: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1987 - accuracy: 0.9330 - val_loss: 0.2360 - val_accuracy: 0.9046\n",
            "Epoch 119/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9320\n",
            "Epoch 119: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.2019 - accuracy: 0.9320 - val_loss: 0.2284 - val_accuracy: 0.9056\n",
            "Epoch 120/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9358\n",
            "Epoch 120: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1950 - accuracy: 0.9358 - val_loss: 0.2262 - val_accuracy: 0.9125\n",
            "Epoch 121/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9377\n",
            "Epoch 121: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1959 - accuracy: 0.9377 - val_loss: 0.2407 - val_accuracy: 0.9036\n",
            "Epoch 122/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9365\n",
            "Epoch 122: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.1937 - accuracy: 0.9365 - val_loss: 0.2192 - val_accuracy: 0.9165\n",
            "Epoch 123/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9355\n",
            "Epoch 123: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1937 - accuracy: 0.9355 - val_loss: 0.2366 - val_accuracy: 0.9056\n",
            "Epoch 124/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9355\n",
            "Epoch 124: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1943 - accuracy: 0.9355 - val_loss: 0.2373 - val_accuracy: 0.9095\n",
            "Epoch 125/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9323\n",
            "Epoch 125: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1941 - accuracy: 0.9323 - val_loss: 0.2261 - val_accuracy: 0.9076\n",
            "Epoch 126/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9308\n",
            "Epoch 126: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1958 - accuracy: 0.9308 - val_loss: 0.2205 - val_accuracy: 0.9175\n",
            "Epoch 127/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9365\n",
            "Epoch 127: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1917 - accuracy: 0.9365 - val_loss: 0.2339 - val_accuracy: 0.9095\n",
            "Epoch 128/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9363\n",
            "Epoch 128: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1880 - accuracy: 0.9363 - val_loss: 0.2159 - val_accuracy: 0.9165\n",
            "Epoch 129/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9365\n",
            "Epoch 129: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1874 - accuracy: 0.9365 - val_loss: 0.2311 - val_accuracy: 0.9105\n",
            "Epoch 130/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9328\n",
            "Epoch 130: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1951 - accuracy: 0.9328 - val_loss: 0.2250 - val_accuracy: 0.9125\n",
            "Epoch 131/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9353\n",
            "Epoch 131: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1870 - accuracy: 0.9353 - val_loss: 0.2328 - val_accuracy: 0.9076\n",
            "Epoch 132/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9363\n",
            "Epoch 132: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1868 - accuracy: 0.9363 - val_loss: 0.2189 - val_accuracy: 0.9085\n",
            "Epoch 133/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9387\n",
            "Epoch 133: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1828 - accuracy: 0.9387 - val_loss: 0.2382 - val_accuracy: 0.9066\n",
            "Epoch 134/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1809 - accuracy: 0.9370\n",
            "Epoch 134: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1809 - accuracy: 0.9370 - val_loss: 0.2151 - val_accuracy: 0.9115\n",
            "Epoch 135/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9440\n",
            "Epoch 135: val_accuracy did not improve from 0.91849\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1787 - accuracy: 0.9440 - val_loss: 0.2283 - val_accuracy: 0.9145\n",
            "Epoch 136/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9415\n",
            "Epoch 136: val_accuracy improved from 0.91849 to 0.92445, saving model to model_save/weights-136-0.9245.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1789 - accuracy: 0.9415 - val_loss: 0.2093 - val_accuracy: 0.9245\n",
            "Epoch 137/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9422\n",
            "Epoch 137: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1810 - accuracy: 0.9422 - val_loss: 0.2309 - val_accuracy: 0.9145\n",
            "Epoch 138/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1772 - accuracy: 0.9440\n",
            "Epoch 138: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1772 - accuracy: 0.9440 - val_loss: 0.2174 - val_accuracy: 0.9155\n",
            "Epoch 139/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9447\n",
            "Epoch 139: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1701 - accuracy: 0.9447 - val_loss: 0.2382 - val_accuracy: 0.9076\n",
            "Epoch 140/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9430\n",
            "Epoch 140: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1774 - accuracy: 0.9430 - val_loss: 0.2204 - val_accuracy: 0.9185\n",
            "Epoch 141/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9363\n",
            "Epoch 141: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1800 - accuracy: 0.9363 - val_loss: 0.2316 - val_accuracy: 0.9085\n",
            "Epoch 142/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9437\n",
            "Epoch 142: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1712 - accuracy: 0.9437 - val_loss: 0.2188 - val_accuracy: 0.9205\n",
            "Epoch 143/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9465\n",
            "Epoch 143: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1699 - accuracy: 0.9465 - val_loss: 0.2205 - val_accuracy: 0.9165\n",
            "Epoch 144/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9457\n",
            "Epoch 144: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1731 - accuracy: 0.9457 - val_loss: 0.2236 - val_accuracy: 0.9195\n",
            "Epoch 145/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9422\n",
            "Epoch 145: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.1722 - accuracy: 0.9422 - val_loss: 0.2224 - val_accuracy: 0.9145\n",
            "Epoch 146/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1696 - accuracy: 0.9430\n",
            "Epoch 146: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1696 - accuracy: 0.9430 - val_loss: 0.2186 - val_accuracy: 0.9165\n",
            "Epoch 147/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9440\n",
            "Epoch 147: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1724 - accuracy: 0.9440 - val_loss: 0.2229 - val_accuracy: 0.9135\n",
            "Epoch 148/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9485\n",
            "Epoch 148: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1691 - accuracy: 0.9485 - val_loss: 0.2093 - val_accuracy: 0.9095\n",
            "Epoch 149/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9437\n",
            "Epoch 149: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1716 - accuracy: 0.9437 - val_loss: 0.2138 - val_accuracy: 0.9115\n",
            "Epoch 150/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9452\n",
            "Epoch 150: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1687 - accuracy: 0.9452 - val_loss: 0.2216 - val_accuracy: 0.9155\n",
            "Epoch 151/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9435\n",
            "Epoch 151: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1682 - accuracy: 0.9435 - val_loss: 0.2346 - val_accuracy: 0.9105\n",
            "Epoch 152/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9430\n",
            "Epoch 152: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1711 - accuracy: 0.9430 - val_loss: 0.2263 - val_accuracy: 0.9095\n",
            "Epoch 153/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9500\n",
            "Epoch 153: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1657 - accuracy: 0.9500 - val_loss: 0.2178 - val_accuracy: 0.9165\n",
            "Epoch 154/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1612 - accuracy: 0.9470\n",
            "Epoch 154: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1612 - accuracy: 0.9470 - val_loss: 0.2109 - val_accuracy: 0.9195\n",
            "Epoch 155/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.9482\n",
            "Epoch 155: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1619 - accuracy: 0.9482 - val_loss: 0.2200 - val_accuracy: 0.9076\n",
            "Epoch 156/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9470\n",
            "Epoch 156: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1678 - accuracy: 0.9470 - val_loss: 0.2050 - val_accuracy: 0.9205\n",
            "Epoch 157/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9460\n",
            "Epoch 157: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1615 - accuracy: 0.9460 - val_loss: 0.2262 - val_accuracy: 0.9215\n",
            "Epoch 158/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.9497\n",
            "Epoch 158: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1592 - accuracy: 0.9497 - val_loss: 0.2261 - val_accuracy: 0.9095\n",
            "Epoch 159/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9507\n",
            "Epoch 159: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1559 - accuracy: 0.9507 - val_loss: 0.2044 - val_accuracy: 0.9235\n",
            "Epoch 160/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9492\n",
            "Epoch 160: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1547 - accuracy: 0.9492 - val_loss: 0.2248 - val_accuracy: 0.9125\n",
            "Epoch 161/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9495\n",
            "Epoch 161: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1586 - accuracy: 0.9495 - val_loss: 0.2309 - val_accuracy: 0.9145\n",
            "Epoch 162/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9457\n",
            "Epoch 162: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1614 - accuracy: 0.9457 - val_loss: 0.2158 - val_accuracy: 0.9205\n",
            "Epoch 163/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9495\n",
            "Epoch 163: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1575 - accuracy: 0.9495 - val_loss: 0.1996 - val_accuracy: 0.9205\n",
            "Epoch 164/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9502\n",
            "Epoch 164: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1551 - accuracy: 0.9502 - val_loss: 0.2337 - val_accuracy: 0.9095\n",
            "Epoch 165/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9430\n",
            "Epoch 165: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1627 - accuracy: 0.9430 - val_loss: 0.2164 - val_accuracy: 0.9185\n",
            "Epoch 166/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9524\n",
            "Epoch 166: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1519 - accuracy: 0.9524 - val_loss: 0.2027 - val_accuracy: 0.9245\n",
            "Epoch 167/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9524\n",
            "Epoch 167: val_accuracy did not improve from 0.92445\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1482 - accuracy: 0.9524 - val_loss: 0.2072 - val_accuracy: 0.9245\n",
            "Epoch 168/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9497\n",
            "Epoch 168: val_accuracy improved from 0.92445 to 0.92644, saving model to model_save/weights-168-0.9264.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1536 - accuracy: 0.9497 - val_loss: 0.2117 - val_accuracy: 0.9264\n",
            "Epoch 169/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1493 - accuracy: 0.9517\n",
            "Epoch 169: val_accuracy did not improve from 0.92644\n",
            "63/63 [==============================] - 8s 120ms/step - loss: 0.1493 - accuracy: 0.9517 - val_loss: 0.2098 - val_accuracy: 0.9205\n",
            "Epoch 170/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9522\n",
            "Epoch 170: val_accuracy improved from 0.92644 to 0.93241, saving model to model_save/weights-170-0.9324.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1500 - accuracy: 0.9522 - val_loss: 0.1942 - val_accuracy: 0.9324\n",
            "Epoch 171/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9527\n",
            "Epoch 171: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1484 - accuracy: 0.9527 - val_loss: 0.2160 - val_accuracy: 0.9274\n",
            "Epoch 172/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9487\n",
            "Epoch 172: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1566 - accuracy: 0.9487 - val_loss: 0.2115 - val_accuracy: 0.9235\n",
            "Epoch 173/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1550 - accuracy: 0.9504\n",
            "Epoch 173: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1550 - accuracy: 0.9504 - val_loss: 0.1963 - val_accuracy: 0.9264\n",
            "Epoch 174/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9529\n",
            "Epoch 174: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1449 - accuracy: 0.9529 - val_loss: 0.2075 - val_accuracy: 0.9225\n",
            "Epoch 175/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9509\n",
            "Epoch 175: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1437 - accuracy: 0.9509 - val_loss: 0.2064 - val_accuracy: 0.9195\n",
            "Epoch 176/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9522\n",
            "Epoch 176: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1505 - accuracy: 0.9522 - val_loss: 0.2101 - val_accuracy: 0.9195\n",
            "Epoch 177/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9500\n",
            "Epoch 177: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1525 - accuracy: 0.9500 - val_loss: 0.2119 - val_accuracy: 0.9235\n",
            "Epoch 178/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.9557\n",
            "Epoch 178: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1441 - accuracy: 0.9557 - val_loss: 0.1993 - val_accuracy: 0.9294\n",
            "Epoch 179/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9534\n",
            "Epoch 179: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1474 - accuracy: 0.9534 - val_loss: 0.2057 - val_accuracy: 0.9205\n",
            "Epoch 180/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9539\n",
            "Epoch 180: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1423 - accuracy: 0.9539 - val_loss: 0.2237 - val_accuracy: 0.9125\n",
            "Epoch 181/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9462\n",
            "Epoch 181: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.1508 - accuracy: 0.9462 - val_loss: 0.2043 - val_accuracy: 0.9225\n",
            "Epoch 182/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9509\n",
            "Epoch 182: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1467 - accuracy: 0.9509 - val_loss: 0.1998 - val_accuracy: 0.9284\n",
            "Epoch 183/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9544\n",
            "Epoch 183: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1467 - accuracy: 0.9544 - val_loss: 0.2180 - val_accuracy: 0.9284\n",
            "Epoch 184/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9532\n",
            "Epoch 184: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1451 - accuracy: 0.9532 - val_loss: 0.2070 - val_accuracy: 0.9264\n",
            "Epoch 185/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9519\n",
            "Epoch 185: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1398 - accuracy: 0.9519 - val_loss: 0.2021 - val_accuracy: 0.9225\n",
            "Epoch 186/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9542\n",
            "Epoch 186: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1434 - accuracy: 0.9542 - val_loss: 0.2333 - val_accuracy: 0.9066\n",
            "Epoch 187/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9534\n",
            "Epoch 187: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1384 - accuracy: 0.9534 - val_loss: 0.2186 - val_accuracy: 0.9235\n",
            "Epoch 188/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9539\n",
            "Epoch 188: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1403 - accuracy: 0.9539 - val_loss: 0.1937 - val_accuracy: 0.9284\n",
            "Epoch 189/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9559\n",
            "Epoch 189: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1423 - accuracy: 0.9559 - val_loss: 0.2193 - val_accuracy: 0.9235\n",
            "Epoch 190/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9552\n",
            "Epoch 190: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1392 - accuracy: 0.9552 - val_loss: 0.2097 - val_accuracy: 0.9175\n",
            "Epoch 191/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1343 - accuracy: 0.9572\n",
            "Epoch 191: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1343 - accuracy: 0.9572 - val_loss: 0.1976 - val_accuracy: 0.9324\n",
            "Epoch 192/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9574\n",
            "Epoch 192: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1379 - accuracy: 0.9574 - val_loss: 0.2078 - val_accuracy: 0.9235\n",
            "Epoch 193/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9549\n",
            "Epoch 193: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1366 - accuracy: 0.9549 - val_loss: 0.2046 - val_accuracy: 0.9205\n",
            "Epoch 194/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9549\n",
            "Epoch 194: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1381 - accuracy: 0.9549 - val_loss: 0.1945 - val_accuracy: 0.9264\n",
            "Epoch 195/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9589\n",
            "Epoch 195: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1347 - accuracy: 0.9589 - val_loss: 0.2075 - val_accuracy: 0.9235\n",
            "Epoch 196/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9592\n",
            "Epoch 196: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1338 - accuracy: 0.9592 - val_loss: 0.1914 - val_accuracy: 0.9225\n",
            "Epoch 197/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1418 - accuracy: 0.9537\n",
            "Epoch 197: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1418 - accuracy: 0.9537 - val_loss: 0.2038 - val_accuracy: 0.9294\n",
            "Epoch 198/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9554\n",
            "Epoch 198: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1381 - accuracy: 0.9554 - val_loss: 0.2116 - val_accuracy: 0.9264\n",
            "Epoch 199/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9564\n",
            "Epoch 199: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1322 - accuracy: 0.9564 - val_loss: 0.2130 - val_accuracy: 0.9155\n",
            "Epoch 200/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9609\n",
            "Epoch 200: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1296 - accuracy: 0.9609 - val_loss: 0.2033 - val_accuracy: 0.9284\n",
            "Epoch 201/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9579\n",
            "Epoch 201: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1344 - accuracy: 0.9579 - val_loss: 0.2118 - val_accuracy: 0.9155\n",
            "Epoch 202/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9557\n",
            "Epoch 202: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1359 - accuracy: 0.9557 - val_loss: 0.2024 - val_accuracy: 0.9235\n",
            "Epoch 203/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9562\n",
            "Epoch 203: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1361 - accuracy: 0.9562 - val_loss: 0.2108 - val_accuracy: 0.9225\n",
            "Epoch 204/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9594\n",
            "Epoch 204: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 8s 120ms/step - loss: 0.1298 - accuracy: 0.9594 - val_loss: 0.2226 - val_accuracy: 0.9215\n",
            "Epoch 205/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9564\n",
            "Epoch 205: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1338 - accuracy: 0.9564 - val_loss: 0.2178 - val_accuracy: 0.9254\n",
            "Epoch 206/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9614\n",
            "Epoch 206: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1222 - accuracy: 0.9614 - val_loss: 0.1974 - val_accuracy: 0.9274\n",
            "Epoch 207/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9641\n",
            "Epoch 207: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1246 - accuracy: 0.9641 - val_loss: 0.2038 - val_accuracy: 0.9245\n",
            "Epoch 208/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9607\n",
            "Epoch 208: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1261 - accuracy: 0.9607 - val_loss: 0.2129 - val_accuracy: 0.9195\n",
            "Epoch 209/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9587\n",
            "Epoch 209: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1311 - accuracy: 0.9587 - val_loss: 0.1918 - val_accuracy: 0.9245\n",
            "Epoch 210/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9549\n",
            "Epoch 210: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1364 - accuracy: 0.9549 - val_loss: 0.2116 - val_accuracy: 0.9185\n",
            "Epoch 211/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9567\n",
            "Epoch 211: val_accuracy did not improve from 0.93241\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1363 - accuracy: 0.9567 - val_loss: 0.2077 - val_accuracy: 0.9235\n",
            "Epoch 212/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1312 - accuracy: 0.9577\n",
            "Epoch 212: val_accuracy improved from 0.93241 to 0.93340, saving model to model_save/weights-212-0.9334.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1312 - accuracy: 0.9577 - val_loss: 0.1917 - val_accuracy: 0.9334\n",
            "Epoch 213/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9577\n",
            "Epoch 213: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1283 - accuracy: 0.9577 - val_loss: 0.2055 - val_accuracy: 0.9284\n",
            "Epoch 214/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9594\n",
            "Epoch 214: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1279 - accuracy: 0.9594 - val_loss: 0.1948 - val_accuracy: 0.9235\n",
            "Epoch 215/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9599\n",
            "Epoch 215: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1247 - accuracy: 0.9599 - val_loss: 0.2047 - val_accuracy: 0.9185\n",
            "Epoch 216/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9587\n",
            "Epoch 216: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1260 - accuracy: 0.9587 - val_loss: 0.1974 - val_accuracy: 0.9294\n",
            "Epoch 217/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9624\n",
            "Epoch 217: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1280 - accuracy: 0.9624 - val_loss: 0.2036 - val_accuracy: 0.9274\n",
            "Epoch 218/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9624\n",
            "Epoch 218: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1222 - accuracy: 0.9624 - val_loss: 0.1972 - val_accuracy: 0.9324\n",
            "Epoch 219/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9617\n",
            "Epoch 219: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1221 - accuracy: 0.9617 - val_loss: 0.1849 - val_accuracy: 0.9304\n",
            "Epoch 220/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9597\n",
            "Epoch 220: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1281 - accuracy: 0.9597 - val_loss: 0.1940 - val_accuracy: 0.9264\n",
            "Epoch 221/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9592\n",
            "Epoch 221: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1245 - accuracy: 0.9592 - val_loss: 0.2032 - val_accuracy: 0.9235\n",
            "Epoch 222/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9604\n",
            "Epoch 222: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1242 - accuracy: 0.9604 - val_loss: 0.2309 - val_accuracy: 0.9165\n",
            "Epoch 223/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9624\n",
            "Epoch 223: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1263 - accuracy: 0.9624 - val_loss: 0.1935 - val_accuracy: 0.9274\n",
            "Epoch 224/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9607\n",
            "Epoch 224: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1227 - accuracy: 0.9607 - val_loss: 0.2167 - val_accuracy: 0.9165\n",
            "Epoch 225/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9587\n",
            "Epoch 225: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1275 - accuracy: 0.9587 - val_loss: 0.2228 - val_accuracy: 0.9115\n",
            "Epoch 226/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9602\n",
            "Epoch 226: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1235 - accuracy: 0.9602 - val_loss: 0.2040 - val_accuracy: 0.9245\n",
            "Epoch 227/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9624\n",
            "Epoch 227: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1203 - accuracy: 0.9624 - val_loss: 0.2004 - val_accuracy: 0.9254\n",
            "Epoch 228/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9641\n",
            "Epoch 228: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1151 - accuracy: 0.9641 - val_loss: 0.1941 - val_accuracy: 0.9304\n",
            "Epoch 229/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9639\n",
            "Epoch 229: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1258 - accuracy: 0.9639 - val_loss: 0.1920 - val_accuracy: 0.9264\n",
            "Epoch 230/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9612\n",
            "Epoch 230: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1198 - accuracy: 0.9612 - val_loss: 0.1996 - val_accuracy: 0.9274\n",
            "Epoch 231/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9636\n",
            "Epoch 231: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1156 - accuracy: 0.9636 - val_loss: 0.2180 - val_accuracy: 0.9215\n",
            "Epoch 232/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9609\n",
            "Epoch 232: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1183 - accuracy: 0.9609 - val_loss: 0.1963 - val_accuracy: 0.9314\n",
            "Epoch 233/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9636\n",
            "Epoch 233: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1174 - accuracy: 0.9636 - val_loss: 0.2330 - val_accuracy: 0.9095\n",
            "Epoch 234/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9599\n",
            "Epoch 234: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1242 - accuracy: 0.9599 - val_loss: 0.2076 - val_accuracy: 0.9254\n",
            "Epoch 235/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9651\n",
            "Epoch 235: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1156 - accuracy: 0.9651 - val_loss: 0.2042 - val_accuracy: 0.9195\n",
            "Epoch 236/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9589\n",
            "Epoch 236: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1223 - accuracy: 0.9589 - val_loss: 0.1975 - val_accuracy: 0.9245\n",
            "Epoch 237/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9636\n",
            "Epoch 237: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1167 - accuracy: 0.9636 - val_loss: 0.1838 - val_accuracy: 0.9284\n",
            "Epoch 238/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9636\n",
            "Epoch 238: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1134 - accuracy: 0.9636 - val_loss: 0.1932 - val_accuracy: 0.9284\n",
            "Epoch 239/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9649\n",
            "Epoch 239: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1148 - accuracy: 0.9649 - val_loss: 0.2099 - val_accuracy: 0.9254\n",
            "Epoch 240/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9629\n",
            "Epoch 240: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1145 - accuracy: 0.9629 - val_loss: 0.2029 - val_accuracy: 0.9314\n",
            "Epoch 241/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9641\n",
            "Epoch 241: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1143 - accuracy: 0.9641 - val_loss: 0.2032 - val_accuracy: 0.9245\n",
            "Epoch 242/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9619\n",
            "Epoch 242: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1162 - accuracy: 0.9619 - val_loss: 0.1944 - val_accuracy: 0.9294\n",
            "Epoch 243/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9626\n",
            "Epoch 243: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1163 - accuracy: 0.9626 - val_loss: 0.2046 - val_accuracy: 0.9235\n",
            "Epoch 244/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9664\n",
            "Epoch 244: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1099 - accuracy: 0.9664 - val_loss: 0.2061 - val_accuracy: 0.9215\n",
            "Epoch 245/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9669\n",
            "Epoch 245: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1092 - accuracy: 0.9669 - val_loss: 0.2056 - val_accuracy: 0.9274\n",
            "Epoch 246/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9684\n",
            "Epoch 246: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1114 - accuracy: 0.9684 - val_loss: 0.2133 - val_accuracy: 0.9245\n",
            "Epoch 247/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9644\n",
            "Epoch 247: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1109 - accuracy: 0.9644 - val_loss: 0.2185 - val_accuracy: 0.9225\n",
            "Epoch 248/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9622\n",
            "Epoch 248: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1150 - accuracy: 0.9622 - val_loss: 0.2038 - val_accuracy: 0.9314\n",
            "Epoch 249/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9631\n",
            "Epoch 249: val_accuracy did not improve from 0.93340\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1156 - accuracy: 0.9631 - val_loss: 0.2248 - val_accuracy: 0.9215\n",
            "Epoch 250/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9639\n",
            "Epoch 250: val_accuracy improved from 0.93340 to 0.93439, saving model to model_save/weights-250-0.9344.hdf5\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1169 - accuracy: 0.9639 - val_loss: 0.1890 - val_accuracy: 0.9344\n",
            "Epoch 251/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9636\n",
            "Epoch 251: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1094 - accuracy: 0.9636 - val_loss: 0.1955 - val_accuracy: 0.9225\n",
            "Epoch 252/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9629\n",
            "Epoch 252: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1133 - accuracy: 0.9629 - val_loss: 0.2174 - val_accuracy: 0.9195\n",
            "Epoch 253/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9636\n",
            "Epoch 253: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1148 - accuracy: 0.9636 - val_loss: 0.2070 - val_accuracy: 0.9254\n",
            "Epoch 254/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9619\n",
            "Epoch 254: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1144 - accuracy: 0.9619 - val_loss: 0.1887 - val_accuracy: 0.9314\n",
            "Epoch 255/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9641\n",
            "Epoch 255: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1104 - accuracy: 0.9641 - val_loss: 0.1826 - val_accuracy: 0.9294\n",
            "Epoch 256/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9696\n",
            "Epoch 256: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1062 - accuracy: 0.9696 - val_loss: 0.1924 - val_accuracy: 0.9304\n",
            "Epoch 257/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9646\n",
            "Epoch 257: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1114 - accuracy: 0.9646 - val_loss: 0.2010 - val_accuracy: 0.9205\n",
            "Epoch 258/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9659\n",
            "Epoch 258: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1052 - accuracy: 0.9659 - val_loss: 0.1977 - val_accuracy: 0.9264\n",
            "Epoch 259/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9609\n",
            "Epoch 259: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1120 - accuracy: 0.9609 - val_loss: 0.1883 - val_accuracy: 0.9304\n",
            "Epoch 260/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9656\n",
            "Epoch 260: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1035 - accuracy: 0.9656 - val_loss: 0.2125 - val_accuracy: 0.9254\n",
            "Epoch 261/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9661\n",
            "Epoch 261: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1097 - accuracy: 0.9661 - val_loss: 0.2030 - val_accuracy: 0.9324\n",
            "Epoch 262/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9666\n",
            "Epoch 262: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1069 - accuracy: 0.9666 - val_loss: 0.1900 - val_accuracy: 0.9304\n",
            "Epoch 263/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9676\n",
            "Epoch 263: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1032 - accuracy: 0.9676 - val_loss: 0.2028 - val_accuracy: 0.9185\n",
            "Epoch 264/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9622\n",
            "Epoch 264: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1149 - accuracy: 0.9622 - val_loss: 0.2098 - val_accuracy: 0.9294\n",
            "Epoch 265/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9669\n",
            "Epoch 265: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1114 - accuracy: 0.9669 - val_loss: 0.1891 - val_accuracy: 0.9334\n",
            "Epoch 266/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9684\n",
            "Epoch 266: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1036 - accuracy: 0.9684 - val_loss: 0.2132 - val_accuracy: 0.9254\n",
            "Epoch 267/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9651\n",
            "Epoch 267: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1055 - accuracy: 0.9651 - val_loss: 0.2038 - val_accuracy: 0.9245\n",
            "Epoch 268/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9681\n",
            "Epoch 268: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1066 - accuracy: 0.9681 - val_loss: 0.2108 - val_accuracy: 0.9254\n",
            "Epoch 269/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1111 - accuracy: 0.9631\n",
            "Epoch 269: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1111 - accuracy: 0.9631 - val_loss: 0.2106 - val_accuracy: 0.9225\n",
            "Epoch 270/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9684\n",
            "Epoch 270: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1076 - accuracy: 0.9684 - val_loss: 0.2199 - val_accuracy: 0.9274\n",
            "Epoch 271/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9661\n",
            "Epoch 271: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1070 - accuracy: 0.9661 - val_loss: 0.2030 - val_accuracy: 0.9235\n",
            "Epoch 272/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9711\n",
            "Epoch 272: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1002 - accuracy: 0.9711 - val_loss: 0.1965 - val_accuracy: 0.9304\n",
            "Epoch 273/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9681\n",
            "Epoch 273: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1052 - accuracy: 0.9681 - val_loss: 0.1941 - val_accuracy: 0.9314\n",
            "Epoch 274/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9676\n",
            "Epoch 274: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1052 - accuracy: 0.9676 - val_loss: 0.1813 - val_accuracy: 0.9294\n",
            "Epoch 275/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9664\n",
            "Epoch 275: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1031 - accuracy: 0.9664 - val_loss: 0.1948 - val_accuracy: 0.9254\n",
            "Epoch 276/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9676\n",
            "Epoch 276: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1051 - accuracy: 0.9676 - val_loss: 0.2006 - val_accuracy: 0.9274\n",
            "Epoch 277/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9669\n",
            "Epoch 277: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1068 - accuracy: 0.9669 - val_loss: 0.1910 - val_accuracy: 0.9254\n",
            "Epoch 278/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9701\n",
            "Epoch 278: val_accuracy did not improve from 0.93439\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1021 - accuracy: 0.9701 - val_loss: 0.1860 - val_accuracy: 0.9324\n",
            "Epoch 279/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9681\n",
            "Epoch 279: val_accuracy improved from 0.93439 to 0.93539, saving model to model_save/weights-279-0.9354.hdf5\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1049 - accuracy: 0.9681 - val_loss: 0.2035 - val_accuracy: 0.9354\n",
            "Epoch 280/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9681\n",
            "Epoch 280: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1019 - accuracy: 0.9681 - val_loss: 0.1990 - val_accuracy: 0.9324\n",
            "Epoch 281/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9646\n",
            "Epoch 281: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1040 - accuracy: 0.9646 - val_loss: 0.2129 - val_accuracy: 0.9284\n",
            "Epoch 282/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9676\n",
            "Epoch 282: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1023 - accuracy: 0.9676 - val_loss: 0.2032 - val_accuracy: 0.9304\n",
            "Epoch 283/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9679\n",
            "Epoch 283: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1114 - accuracy: 0.9679 - val_loss: 0.2007 - val_accuracy: 0.9264\n",
            "Epoch 284/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9636\n",
            "Epoch 284: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1026 - accuracy: 0.9636 - val_loss: 0.1902 - val_accuracy: 0.9314\n",
            "Epoch 285/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9714\n",
            "Epoch 285: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1003 - accuracy: 0.9714 - val_loss: 0.2156 - val_accuracy: 0.9155\n",
            "Epoch 286/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9661\n",
            "Epoch 286: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1026 - accuracy: 0.9661 - val_loss: 0.1991 - val_accuracy: 0.9324\n",
            "Epoch 287/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9644\n",
            "Epoch 287: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1062 - accuracy: 0.9644 - val_loss: 0.1988 - val_accuracy: 0.9284\n",
            "Epoch 288/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9711\n",
            "Epoch 288: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0976 - accuracy: 0.9711 - val_loss: 0.2118 - val_accuracy: 0.9155\n",
            "Epoch 289/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9674\n",
            "Epoch 289: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.1027 - accuracy: 0.9674 - val_loss: 0.1940 - val_accuracy: 0.9324\n",
            "Epoch 290/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.9689\n",
            "Epoch 290: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0995 - accuracy: 0.9689 - val_loss: 0.1965 - val_accuracy: 0.9264\n",
            "Epoch 291/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9691\n",
            "Epoch 291: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0969 - accuracy: 0.9691 - val_loss: 0.1908 - val_accuracy: 0.9314\n",
            "Epoch 292/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9659\n",
            "Epoch 292: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1030 - accuracy: 0.9659 - val_loss: 0.1847 - val_accuracy: 0.9225\n",
            "Epoch 293/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9684\n",
            "Epoch 293: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1009 - accuracy: 0.9684 - val_loss: 0.2142 - val_accuracy: 0.9205\n",
            "Epoch 294/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9671\n",
            "Epoch 294: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1008 - accuracy: 0.9671 - val_loss: 0.1946 - val_accuracy: 0.9284\n",
            "Epoch 295/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9649\n",
            "Epoch 295: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0991 - accuracy: 0.9649 - val_loss: 0.1843 - val_accuracy: 0.9354\n",
            "Epoch 296/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9691\n",
            "Epoch 296: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0990 - accuracy: 0.9691 - val_loss: 0.2067 - val_accuracy: 0.9284\n",
            "Epoch 297/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9739\n",
            "Epoch 297: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0924 - accuracy: 0.9739 - val_loss: 0.1827 - val_accuracy: 0.9334\n",
            "Epoch 298/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9696\n",
            "Epoch 298: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.1006 - accuracy: 0.9696 - val_loss: 0.1942 - val_accuracy: 0.9294\n",
            "Epoch 299/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9694\n",
            "Epoch 299: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0999 - accuracy: 0.9694 - val_loss: 0.2076 - val_accuracy: 0.9245\n",
            "Epoch 300/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9719\n",
            "Epoch 300: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0919 - accuracy: 0.9719 - val_loss: 0.1944 - val_accuracy: 0.9314\n",
            "Epoch 301/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9719\n",
            "Epoch 301: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0978 - accuracy: 0.9719 - val_loss: 0.2070 - val_accuracy: 0.9264\n",
            "Epoch 302/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9729\n",
            "Epoch 302: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0953 - accuracy: 0.9729 - val_loss: 0.1894 - val_accuracy: 0.9245\n",
            "Epoch 303/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9704\n",
            "Epoch 303: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0947 - accuracy: 0.9704 - val_loss: 0.2020 - val_accuracy: 0.9354\n",
            "Epoch 304/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9691\n",
            "Epoch 304: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0988 - accuracy: 0.9691 - val_loss: 0.1857 - val_accuracy: 0.9324\n",
            "Epoch 305/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9646\n",
            "Epoch 305: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0990 - accuracy: 0.9646 - val_loss: 0.1887 - val_accuracy: 0.9324\n",
            "Epoch 306/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9691\n",
            "Epoch 306: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 114ms/step - loss: 0.0983 - accuracy: 0.9691 - val_loss: 0.1909 - val_accuracy: 0.9304\n",
            "Epoch 307/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9644\n",
            "Epoch 307: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.1038 - accuracy: 0.9644 - val_loss: 0.2156 - val_accuracy: 0.9215\n",
            "Epoch 308/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9704\n",
            "Epoch 308: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0967 - accuracy: 0.9704 - val_loss: 0.1998 - val_accuracy: 0.9264\n",
            "Epoch 309/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9684\n",
            "Epoch 309: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.0961 - accuracy: 0.9684 - val_loss: 0.2156 - val_accuracy: 0.9245\n",
            "Epoch 310/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9714\n",
            "Epoch 310: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0923 - accuracy: 0.9714 - val_loss: 0.2056 - val_accuracy: 0.9215\n",
            "Epoch 311/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9711\n",
            "Epoch 311: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0944 - accuracy: 0.9711 - val_loss: 0.2040 - val_accuracy: 0.9254\n",
            "Epoch 312/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9734\n",
            "Epoch 312: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0912 - accuracy: 0.9734 - val_loss: 0.2145 - val_accuracy: 0.9215\n",
            "Epoch 313/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9681\n",
            "Epoch 313: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1010 - accuracy: 0.9681 - val_loss: 0.2001 - val_accuracy: 0.9175\n",
            "Epoch 314/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9726\n",
            "Epoch 314: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0932 - accuracy: 0.9726 - val_loss: 0.1985 - val_accuracy: 0.9225\n",
            "Epoch 315/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9666\n",
            "Epoch 315: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0979 - accuracy: 0.9666 - val_loss: 0.2128 - val_accuracy: 0.9185\n",
            "Epoch 316/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9729\n",
            "Epoch 316: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0903 - accuracy: 0.9729 - val_loss: 0.2002 - val_accuracy: 0.9245\n",
            "Epoch 317/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9739\n",
            "Epoch 317: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0971 - accuracy: 0.9739 - val_loss: 0.1962 - val_accuracy: 0.9225\n",
            "Epoch 318/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9656\n",
            "Epoch 318: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.1046 - accuracy: 0.9656 - val_loss: 0.2051 - val_accuracy: 0.9324\n",
            "Epoch 319/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9701\n",
            "Epoch 319: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0924 - accuracy: 0.9701 - val_loss: 0.2133 - val_accuracy: 0.9215\n",
            "Epoch 320/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9699\n",
            "Epoch 320: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.0991 - accuracy: 0.9699 - val_loss: 0.1956 - val_accuracy: 0.9284\n",
            "Epoch 321/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9681\n",
            "Epoch 321: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0956 - accuracy: 0.9681 - val_loss: 0.2030 - val_accuracy: 0.9274\n",
            "Epoch 322/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9726\n",
            "Epoch 322: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0969 - accuracy: 0.9726 - val_loss: 0.2122 - val_accuracy: 0.9304\n",
            "Epoch 323/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9699\n",
            "Epoch 323: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0957 - accuracy: 0.9699 - val_loss: 0.1878 - val_accuracy: 0.9284\n",
            "Epoch 324/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9736\n",
            "Epoch 324: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0959 - accuracy: 0.9736 - val_loss: 0.1923 - val_accuracy: 0.9334\n",
            "Epoch 325/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9731\n",
            "Epoch 325: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0909 - accuracy: 0.9731 - val_loss: 0.2213 - val_accuracy: 0.9205\n",
            "Epoch 326/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9674\n",
            "Epoch 326: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0959 - accuracy: 0.9674 - val_loss: 0.2272 - val_accuracy: 0.9205\n",
            "Epoch 327/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9771\n",
            "Epoch 327: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0866 - accuracy: 0.9771 - val_loss: 0.2193 - val_accuracy: 0.9175\n",
            "Epoch 328/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9704\n",
            "Epoch 328: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0927 - accuracy: 0.9704 - val_loss: 0.1939 - val_accuracy: 0.9254\n",
            "Epoch 329/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9721\n",
            "Epoch 329: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0895 - accuracy: 0.9721 - val_loss: 0.2037 - val_accuracy: 0.9294\n",
            "Epoch 330/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9714\n",
            "Epoch 330: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0908 - accuracy: 0.9714 - val_loss: 0.2296 - val_accuracy: 0.9205\n",
            "Epoch 331/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9753\n",
            "Epoch 331: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0871 - accuracy: 0.9753 - val_loss: 0.1968 - val_accuracy: 0.9284\n",
            "Epoch 332/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9686\n",
            "Epoch 332: val_accuracy did not improve from 0.93539\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0951 - accuracy: 0.9686 - val_loss: 0.1982 - val_accuracy: 0.9195\n",
            "Epoch 333/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9724\n",
            "Epoch 333: val_accuracy improved from 0.93539 to 0.93936, saving model to model_save/weights-333-0.9394.hdf5\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0879 - accuracy: 0.9724 - val_loss: 0.1825 - val_accuracy: 0.9394\n",
            "Epoch 334/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9709\n",
            "Epoch 334: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0897 - accuracy: 0.9709 - val_loss: 0.2074 - val_accuracy: 0.9235\n",
            "Epoch 335/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9746\n",
            "Epoch 335: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0871 - accuracy: 0.9746 - val_loss: 0.1926 - val_accuracy: 0.9304\n",
            "Epoch 336/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9736\n",
            "Epoch 336: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0875 - accuracy: 0.9736 - val_loss: 0.2136 - val_accuracy: 0.9215\n",
            "Epoch 337/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9701\n",
            "Epoch 337: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0943 - accuracy: 0.9701 - val_loss: 0.2102 - val_accuracy: 0.9225\n",
            "Epoch 338/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9706\n",
            "Epoch 338: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0931 - accuracy: 0.9706 - val_loss: 0.1872 - val_accuracy: 0.9364\n",
            "Epoch 339/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9719\n",
            "Epoch 339: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0920 - accuracy: 0.9719 - val_loss: 0.1953 - val_accuracy: 0.9215\n",
            "Epoch 340/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9701\n",
            "Epoch 340: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0922 - accuracy: 0.9701 - val_loss: 0.1898 - val_accuracy: 0.9334\n",
            "Epoch 341/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9721\n",
            "Epoch 341: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0906 - accuracy: 0.9721 - val_loss: 0.2032 - val_accuracy: 0.9304\n",
            "Epoch 342/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9716\n",
            "Epoch 342: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0900 - accuracy: 0.9716 - val_loss: 0.1981 - val_accuracy: 0.9294\n",
            "Epoch 343/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9729\n",
            "Epoch 343: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.0896 - accuracy: 0.9729 - val_loss: 0.2038 - val_accuracy: 0.9254\n",
            "Epoch 344/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9734\n",
            "Epoch 344: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0899 - accuracy: 0.9734 - val_loss: 0.1932 - val_accuracy: 0.9264\n",
            "Epoch 345/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9744\n",
            "Epoch 345: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0870 - accuracy: 0.9744 - val_loss: 0.1905 - val_accuracy: 0.9304\n",
            "Epoch 346/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9746\n",
            "Epoch 346: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0882 - accuracy: 0.9746 - val_loss: 0.1908 - val_accuracy: 0.9364\n",
            "Epoch 347/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9744\n",
            "Epoch 347: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0863 - accuracy: 0.9744 - val_loss: 0.2294 - val_accuracy: 0.9235\n",
            "Epoch 348/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9714\n",
            "Epoch 348: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0937 - accuracy: 0.9714 - val_loss: 0.1919 - val_accuracy: 0.9334\n",
            "Epoch 349/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9773\n",
            "Epoch 349: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0816 - accuracy: 0.9773 - val_loss: 0.2035 - val_accuracy: 0.9334\n",
            "Epoch 350/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9741\n",
            "Epoch 350: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0903 - accuracy: 0.9741 - val_loss: 0.2123 - val_accuracy: 0.9274\n",
            "Epoch 351/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9749\n",
            "Epoch 351: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0833 - accuracy: 0.9749 - val_loss: 0.1908 - val_accuracy: 0.9294\n",
            "Epoch 352/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9741\n",
            "Epoch 352: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0846 - accuracy: 0.9741 - val_loss: 0.1887 - val_accuracy: 0.9374\n",
            "Epoch 353/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9734\n",
            "Epoch 353: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0824 - accuracy: 0.9734 - val_loss: 0.2109 - val_accuracy: 0.9205\n",
            "Epoch 354/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9696\n",
            "Epoch 354: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0888 - accuracy: 0.9696 - val_loss: 0.2048 - val_accuracy: 0.9254\n",
            "Epoch 355/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9661\n",
            "Epoch 355: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0991 - accuracy: 0.9661 - val_loss: 0.2118 - val_accuracy: 0.9205\n",
            "Epoch 356/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9706\n",
            "Epoch 356: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0893 - accuracy: 0.9706 - val_loss: 0.1938 - val_accuracy: 0.9245\n",
            "Epoch 357/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9753\n",
            "Epoch 357: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0840 - accuracy: 0.9753 - val_loss: 0.1898 - val_accuracy: 0.9304\n",
            "Epoch 358/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9773\n",
            "Epoch 358: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0816 - accuracy: 0.9773 - val_loss: 0.2001 - val_accuracy: 0.9245\n",
            "Epoch 359/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9761\n",
            "Epoch 359: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0851 - accuracy: 0.9761 - val_loss: 0.2111 - val_accuracy: 0.9334\n",
            "Epoch 360/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9724\n",
            "Epoch 360: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0900 - accuracy: 0.9724 - val_loss: 0.2152 - val_accuracy: 0.9215\n",
            "Epoch 361/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9714\n",
            "Epoch 361: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0872 - accuracy: 0.9714 - val_loss: 0.2218 - val_accuracy: 0.9175\n",
            "Epoch 362/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9751\n",
            "Epoch 362: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0869 - accuracy: 0.9751 - val_loss: 0.2084 - val_accuracy: 0.9304\n",
            "Epoch 363/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9736\n",
            "Epoch 363: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0854 - accuracy: 0.9736 - val_loss: 0.1850 - val_accuracy: 0.9364\n",
            "Epoch 364/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9753\n",
            "Epoch 364: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0845 - accuracy: 0.9753 - val_loss: 0.2203 - val_accuracy: 0.9225\n",
            "Epoch 365/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9741\n",
            "Epoch 365: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0803 - accuracy: 0.9741 - val_loss: 0.2091 - val_accuracy: 0.9215\n",
            "Epoch 366/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9734\n",
            "Epoch 366: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0828 - accuracy: 0.9734 - val_loss: 0.2081 - val_accuracy: 0.9304\n",
            "Epoch 367/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9734\n",
            "Epoch 367: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0832 - accuracy: 0.9734 - val_loss: 0.2117 - val_accuracy: 0.9284\n",
            "Epoch 368/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9783\n",
            "Epoch 368: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0773 - accuracy: 0.9783 - val_loss: 0.1910 - val_accuracy: 0.9294\n",
            "Epoch 369/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9731\n",
            "Epoch 369: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0832 - accuracy: 0.9731 - val_loss: 0.1791 - val_accuracy: 0.9314\n",
            "Epoch 370/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9734\n",
            "Epoch 370: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0880 - accuracy: 0.9734 - val_loss: 0.2085 - val_accuracy: 0.9254\n",
            "Epoch 371/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9714\n",
            "Epoch 371: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0856 - accuracy: 0.9714 - val_loss: 0.2086 - val_accuracy: 0.9254\n",
            "Epoch 372/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9684\n",
            "Epoch 372: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0918 - accuracy: 0.9684 - val_loss: 0.2212 - val_accuracy: 0.9145\n",
            "Epoch 373/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9731\n",
            "Epoch 373: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0856 - accuracy: 0.9731 - val_loss: 0.1969 - val_accuracy: 0.9274\n",
            "Epoch 374/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9734\n",
            "Epoch 374: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0838 - accuracy: 0.9734 - val_loss: 0.1901 - val_accuracy: 0.9314\n",
            "Epoch 375/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9696\n",
            "Epoch 375: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0894 - accuracy: 0.9696 - val_loss: 0.2172 - val_accuracy: 0.9225\n",
            "Epoch 376/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9741\n",
            "Epoch 376: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0836 - accuracy: 0.9741 - val_loss: 0.2246 - val_accuracy: 0.9235\n",
            "Epoch 377/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9691\n",
            "Epoch 377: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0895 - accuracy: 0.9691 - val_loss: 0.2057 - val_accuracy: 0.9304\n",
            "Epoch 378/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9746\n",
            "Epoch 378: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0855 - accuracy: 0.9746 - val_loss: 0.2214 - val_accuracy: 0.9195\n",
            "Epoch 379/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9714\n",
            "Epoch 379: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0868 - accuracy: 0.9714 - val_loss: 0.1971 - val_accuracy: 0.9254\n",
            "Epoch 380/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9719\n",
            "Epoch 380: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0851 - accuracy: 0.9719 - val_loss: 0.2043 - val_accuracy: 0.9284\n",
            "Epoch 381/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9736\n",
            "Epoch 381: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0819 - accuracy: 0.9736 - val_loss: 0.1956 - val_accuracy: 0.9304\n",
            "Epoch 382/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9768\n",
            "Epoch 382: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0766 - accuracy: 0.9768 - val_loss: 0.1998 - val_accuracy: 0.9195\n",
            "Epoch 383/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9798\n",
            "Epoch 383: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0743 - accuracy: 0.9798 - val_loss: 0.1943 - val_accuracy: 0.9334\n",
            "Epoch 384/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9756\n",
            "Epoch 384: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0855 - accuracy: 0.9756 - val_loss: 0.2070 - val_accuracy: 0.9264\n",
            "Epoch 385/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9761\n",
            "Epoch 385: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0806 - accuracy: 0.9761 - val_loss: 0.1902 - val_accuracy: 0.9294\n",
            "Epoch 386/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9771\n",
            "Epoch 386: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0811 - accuracy: 0.9771 - val_loss: 0.2385 - val_accuracy: 0.9225\n",
            "Epoch 387/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9736\n",
            "Epoch 387: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0814 - accuracy: 0.9736 - val_loss: 0.2071 - val_accuracy: 0.9314\n",
            "Epoch 388/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9761\n",
            "Epoch 388: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0810 - accuracy: 0.9761 - val_loss: 0.2156 - val_accuracy: 0.9254\n",
            "Epoch 389/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9716\n",
            "Epoch 389: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0831 - accuracy: 0.9716 - val_loss: 0.1951 - val_accuracy: 0.9324\n",
            "Epoch 390/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9726\n",
            "Epoch 390: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0824 - accuracy: 0.9726 - val_loss: 0.2090 - val_accuracy: 0.9235\n",
            "Epoch 391/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9741\n",
            "Epoch 391: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0820 - accuracy: 0.9741 - val_loss: 0.1873 - val_accuracy: 0.9344\n",
            "Epoch 392/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9801\n",
            "Epoch 392: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0758 - accuracy: 0.9801 - val_loss: 0.1938 - val_accuracy: 0.9304\n",
            "Epoch 393/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9773\n",
            "Epoch 393: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0741 - accuracy: 0.9773 - val_loss: 0.2199 - val_accuracy: 0.9235\n",
            "Epoch 394/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9746\n",
            "Epoch 394: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0800 - accuracy: 0.9746 - val_loss: 0.2146 - val_accuracy: 0.9304\n",
            "Epoch 395/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9724\n",
            "Epoch 395: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0845 - accuracy: 0.9724 - val_loss: 0.2100 - val_accuracy: 0.9264\n",
            "Epoch 396/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9706\n",
            "Epoch 396: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0944 - accuracy: 0.9706 - val_loss: 0.1947 - val_accuracy: 0.9274\n",
            "Epoch 397/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9771\n",
            "Epoch 397: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0762 - accuracy: 0.9771 - val_loss: 0.1980 - val_accuracy: 0.9294\n",
            "Epoch 398/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9781\n",
            "Epoch 398: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0783 - accuracy: 0.9781 - val_loss: 0.2178 - val_accuracy: 0.9225\n",
            "Epoch 399/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9729\n",
            "Epoch 399: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0829 - accuracy: 0.9729 - val_loss: 0.1989 - val_accuracy: 0.9314\n",
            "Epoch 400/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9766\n",
            "Epoch 400: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0830 - accuracy: 0.9766 - val_loss: 0.2019 - val_accuracy: 0.9294\n",
            "Epoch 401/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9773\n",
            "Epoch 401: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0802 - accuracy: 0.9773 - val_loss: 0.2124 - val_accuracy: 0.9264\n",
            "Epoch 402/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9768\n",
            "Epoch 402: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0751 - accuracy: 0.9768 - val_loss: 0.1970 - val_accuracy: 0.9254\n",
            "Epoch 403/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9766\n",
            "Epoch 403: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0756 - accuracy: 0.9766 - val_loss: 0.2090 - val_accuracy: 0.9284\n",
            "Epoch 404/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9731\n",
            "Epoch 404: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0805 - accuracy: 0.9731 - val_loss: 0.1941 - val_accuracy: 0.9274\n",
            "Epoch 405/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9746\n",
            "Epoch 405: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0791 - accuracy: 0.9746 - val_loss: 0.2058 - val_accuracy: 0.9235\n",
            "Epoch 406/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9734\n",
            "Epoch 406: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0807 - accuracy: 0.9734 - val_loss: 0.2067 - val_accuracy: 0.9344\n",
            "Epoch 407/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9758\n",
            "Epoch 407: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0768 - accuracy: 0.9758 - val_loss: 0.1968 - val_accuracy: 0.9324\n",
            "Epoch 408/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9776\n",
            "Epoch 408: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0747 - accuracy: 0.9776 - val_loss: 0.2044 - val_accuracy: 0.9225\n",
            "Epoch 409/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9739\n",
            "Epoch 409: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0817 - accuracy: 0.9739 - val_loss: 0.2050 - val_accuracy: 0.9294\n",
            "Epoch 410/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9761\n",
            "Epoch 410: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0796 - accuracy: 0.9761 - val_loss: 0.2025 - val_accuracy: 0.9274\n",
            "Epoch 411/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9751\n",
            "Epoch 411: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0769 - accuracy: 0.9751 - val_loss: 0.2425 - val_accuracy: 0.9095\n",
            "Epoch 412/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9749\n",
            "Epoch 412: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0754 - accuracy: 0.9749 - val_loss: 0.2242 - val_accuracy: 0.9135\n",
            "Epoch 413/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9788\n",
            "Epoch 413: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0731 - accuracy: 0.9788 - val_loss: 0.2116 - val_accuracy: 0.9274\n",
            "Epoch 414/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9739\n",
            "Epoch 414: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0823 - accuracy: 0.9739 - val_loss: 0.1965 - val_accuracy: 0.9324\n",
            "Epoch 415/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9761\n",
            "Epoch 415: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0748 - accuracy: 0.9761 - val_loss: 0.1940 - val_accuracy: 0.9304\n",
            "Epoch 416/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9793\n",
            "Epoch 416: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0727 - accuracy: 0.9793 - val_loss: 0.1945 - val_accuracy: 0.9245\n",
            "Epoch 417/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9749\n",
            "Epoch 417: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0728 - accuracy: 0.9749 - val_loss: 0.2229 - val_accuracy: 0.9205\n",
            "Epoch 418/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9758\n",
            "Epoch 418: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0790 - accuracy: 0.9758 - val_loss: 0.2067 - val_accuracy: 0.9185\n",
            "Epoch 419/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9696\n",
            "Epoch 419: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0899 - accuracy: 0.9696 - val_loss: 0.2405 - val_accuracy: 0.9115\n",
            "Epoch 420/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9731\n",
            "Epoch 420: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0827 - accuracy: 0.9731 - val_loss: 0.2040 - val_accuracy: 0.9235\n",
            "Epoch 421/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9749\n",
            "Epoch 421: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0825 - accuracy: 0.9749 - val_loss: 0.1830 - val_accuracy: 0.9324\n",
            "Epoch 422/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9778\n",
            "Epoch 422: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0785 - accuracy: 0.9778 - val_loss: 0.1795 - val_accuracy: 0.9354\n",
            "Epoch 423/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9766\n",
            "Epoch 423: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0760 - accuracy: 0.9766 - val_loss: 0.2245 - val_accuracy: 0.9304\n",
            "Epoch 424/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9776\n",
            "Epoch 424: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0737 - accuracy: 0.9776 - val_loss: 0.2170 - val_accuracy: 0.9215\n",
            "Epoch 425/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9749\n",
            "Epoch 425: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0840 - accuracy: 0.9749 - val_loss: 0.2052 - val_accuracy: 0.9284\n",
            "Epoch 426/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9729\n",
            "Epoch 426: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 114ms/step - loss: 0.0813 - accuracy: 0.9729 - val_loss: 0.1872 - val_accuracy: 0.9304\n",
            "Epoch 427/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9763\n",
            "Epoch 427: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0763 - accuracy: 0.9763 - val_loss: 0.2081 - val_accuracy: 0.9274\n",
            "Epoch 428/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9758\n",
            "Epoch 428: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 114ms/step - loss: 0.0725 - accuracy: 0.9758 - val_loss: 0.1960 - val_accuracy: 0.9314\n",
            "Epoch 429/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9756\n",
            "Epoch 429: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0761 - accuracy: 0.9756 - val_loss: 0.1995 - val_accuracy: 0.9294\n",
            "Epoch 430/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9768\n",
            "Epoch 430: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0758 - accuracy: 0.9768 - val_loss: 0.2169 - val_accuracy: 0.9195\n",
            "Epoch 431/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9741\n",
            "Epoch 431: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0787 - accuracy: 0.9741 - val_loss: 0.2382 - val_accuracy: 0.9245\n",
            "Epoch 432/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9761\n",
            "Epoch 432: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0774 - accuracy: 0.9761 - val_loss: 0.1923 - val_accuracy: 0.9284\n",
            "Epoch 433/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9781\n",
            "Epoch 433: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0753 - accuracy: 0.9781 - val_loss: 0.2094 - val_accuracy: 0.9254\n",
            "Epoch 434/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9763\n",
            "Epoch 434: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0786 - accuracy: 0.9763 - val_loss: 0.2632 - val_accuracy: 0.9175\n",
            "Epoch 435/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9741\n",
            "Epoch 435: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0804 - accuracy: 0.9741 - val_loss: 0.1817 - val_accuracy: 0.9314\n",
            "Epoch 436/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9709\n",
            "Epoch 436: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0813 - accuracy: 0.9709 - val_loss: 0.2210 - val_accuracy: 0.9215\n",
            "Epoch 437/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9776\n",
            "Epoch 437: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0724 - accuracy: 0.9776 - val_loss: 0.1958 - val_accuracy: 0.9334\n",
            "Epoch 438/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9803\n",
            "Epoch 438: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0678 - accuracy: 0.9803 - val_loss: 0.1934 - val_accuracy: 0.9314\n",
            "Epoch 439/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9729\n",
            "Epoch 439: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0792 - accuracy: 0.9729 - val_loss: 0.2049 - val_accuracy: 0.9215\n",
            "Epoch 440/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9768\n",
            "Epoch 440: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0731 - accuracy: 0.9768 - val_loss: 0.1903 - val_accuracy: 0.9304\n",
            "Epoch 441/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9766\n",
            "Epoch 441: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0733 - accuracy: 0.9766 - val_loss: 0.1948 - val_accuracy: 0.9314\n",
            "Epoch 442/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9766\n",
            "Epoch 442: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0733 - accuracy: 0.9766 - val_loss: 0.1865 - val_accuracy: 0.9354\n",
            "Epoch 443/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9729\n",
            "Epoch 443: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0822 - accuracy: 0.9729 - val_loss: 0.2145 - val_accuracy: 0.9284\n",
            "Epoch 444/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9756\n",
            "Epoch 444: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0747 - accuracy: 0.9756 - val_loss: 0.1961 - val_accuracy: 0.9304\n",
            "Epoch 445/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9756\n",
            "Epoch 445: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0754 - accuracy: 0.9756 - val_loss: 0.1906 - val_accuracy: 0.9254\n",
            "Epoch 446/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9813\n",
            "Epoch 446: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0676 - accuracy: 0.9813 - val_loss: 0.1920 - val_accuracy: 0.9294\n",
            "Epoch 447/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9821\n",
            "Epoch 447: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0693 - accuracy: 0.9821 - val_loss: 0.2009 - val_accuracy: 0.9334\n",
            "Epoch 448/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9801\n",
            "Epoch 448: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0686 - accuracy: 0.9801 - val_loss: 0.2049 - val_accuracy: 0.9304\n",
            "Epoch 449/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9781\n",
            "Epoch 449: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0722 - accuracy: 0.9781 - val_loss: 0.2051 - val_accuracy: 0.9294\n",
            "Epoch 450/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9771\n",
            "Epoch 450: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0739 - accuracy: 0.9771 - val_loss: 0.1872 - val_accuracy: 0.9344\n",
            "Epoch 451/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9788\n",
            "Epoch 451: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0682 - accuracy: 0.9788 - val_loss: 0.2108 - val_accuracy: 0.9274\n",
            "Epoch 452/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9786\n",
            "Epoch 452: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0673 - accuracy: 0.9786 - val_loss: 0.2115 - val_accuracy: 0.9324\n",
            "Epoch 453/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9751\n",
            "Epoch 453: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0771 - accuracy: 0.9751 - val_loss: 0.1950 - val_accuracy: 0.9314\n",
            "Epoch 454/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9793\n",
            "Epoch 454: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0698 - accuracy: 0.9793 - val_loss: 0.1892 - val_accuracy: 0.9215\n",
            "Epoch 455/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9771\n",
            "Epoch 455: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0742 - accuracy: 0.9771 - val_loss: 0.1945 - val_accuracy: 0.9334\n",
            "Epoch 456/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9763\n",
            "Epoch 456: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0808 - accuracy: 0.9763 - val_loss: 0.2091 - val_accuracy: 0.9324\n",
            "Epoch 457/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9773\n",
            "Epoch 457: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0753 - accuracy: 0.9773 - val_loss: 0.2029 - val_accuracy: 0.9245\n",
            "Epoch 458/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9753\n",
            "Epoch 458: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0725 - accuracy: 0.9753 - val_loss: 0.2221 - val_accuracy: 0.9225\n",
            "Epoch 459/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9749\n",
            "Epoch 459: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0775 - accuracy: 0.9749 - val_loss: 0.2061 - val_accuracy: 0.9274\n",
            "Epoch 460/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9778\n",
            "Epoch 460: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0740 - accuracy: 0.9778 - val_loss: 0.1953 - val_accuracy: 0.9314\n",
            "Epoch 461/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9744\n",
            "Epoch 461: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0730 - accuracy: 0.9744 - val_loss: 0.1962 - val_accuracy: 0.9274\n",
            "Epoch 462/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9781\n",
            "Epoch 462: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0728 - accuracy: 0.9781 - val_loss: 0.2147 - val_accuracy: 0.9304\n",
            "Epoch 463/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9798\n",
            "Epoch 463: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0723 - accuracy: 0.9798 - val_loss: 0.1846 - val_accuracy: 0.9344\n",
            "Epoch 464/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9771\n",
            "Epoch 464: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0754 - accuracy: 0.9771 - val_loss: 0.2163 - val_accuracy: 0.9254\n",
            "Epoch 465/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9788\n",
            "Epoch 465: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0741 - accuracy: 0.9788 - val_loss: 0.2436 - val_accuracy: 0.9165\n",
            "Epoch 466/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9766\n",
            "Epoch 466: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0736 - accuracy: 0.9766 - val_loss: 0.2003 - val_accuracy: 0.9284\n",
            "Epoch 467/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9766\n",
            "Epoch 467: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0689 - accuracy: 0.9766 - val_loss: 0.1995 - val_accuracy: 0.9235\n",
            "Epoch 468/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9766\n",
            "Epoch 468: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0703 - accuracy: 0.9766 - val_loss: 0.2338 - val_accuracy: 0.9205\n",
            "Epoch 469/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9771\n",
            "Epoch 469: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0731 - accuracy: 0.9771 - val_loss: 0.2171 - val_accuracy: 0.9254\n",
            "Epoch 470/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9758\n",
            "Epoch 470: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0743 - accuracy: 0.9758 - val_loss: 0.2026 - val_accuracy: 0.9314\n",
            "Epoch 471/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9768\n",
            "Epoch 471: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0758 - accuracy: 0.9768 - val_loss: 0.2050 - val_accuracy: 0.9304\n",
            "Epoch 472/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9758\n",
            "Epoch 472: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0748 - accuracy: 0.9758 - val_loss: 0.2044 - val_accuracy: 0.9264\n",
            "Epoch 473/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9778\n",
            "Epoch 473: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0722 - accuracy: 0.9778 - val_loss: 0.2033 - val_accuracy: 0.9314\n",
            "Epoch 474/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9744\n",
            "Epoch 474: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0796 - accuracy: 0.9744 - val_loss: 0.2234 - val_accuracy: 0.9254\n",
            "Epoch 475/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9783\n",
            "Epoch 475: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0687 - accuracy: 0.9783 - val_loss: 0.1960 - val_accuracy: 0.9274\n",
            "Epoch 476/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9749\n",
            "Epoch 476: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0750 - accuracy: 0.9749 - val_loss: 0.2130 - val_accuracy: 0.9314\n",
            "Epoch 477/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9821\n",
            "Epoch 477: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 119ms/step - loss: 0.0635 - accuracy: 0.9821 - val_loss: 0.2065 - val_accuracy: 0.9264\n",
            "Epoch 478/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9796\n",
            "Epoch 478: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0691 - accuracy: 0.9796 - val_loss: 0.2043 - val_accuracy: 0.9274\n",
            "Epoch 479/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9768\n",
            "Epoch 479: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0704 - accuracy: 0.9768 - val_loss: 0.2154 - val_accuracy: 0.9225\n",
            "Epoch 480/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9793\n",
            "Epoch 480: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0677 - accuracy: 0.9793 - val_loss: 0.2039 - val_accuracy: 0.9314\n",
            "Epoch 481/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9816\n",
            "Epoch 481: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0640 - accuracy: 0.9816 - val_loss: 0.1849 - val_accuracy: 0.9344\n",
            "Epoch 482/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9806\n",
            "Epoch 482: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0699 - accuracy: 0.9806 - val_loss: 0.2069 - val_accuracy: 0.9364\n",
            "Epoch 483/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9793\n",
            "Epoch 483: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0670 - accuracy: 0.9793 - val_loss: 0.1907 - val_accuracy: 0.9394\n",
            "Epoch 484/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9818\n",
            "Epoch 484: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0667 - accuracy: 0.9818 - val_loss: 0.2136 - val_accuracy: 0.9195\n",
            "Epoch 485/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9823\n",
            "Epoch 485: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0618 - accuracy: 0.9823 - val_loss: 0.1997 - val_accuracy: 0.9274\n",
            "Epoch 486/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9776\n",
            "Epoch 486: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0695 - accuracy: 0.9776 - val_loss: 0.2191 - val_accuracy: 0.9195\n",
            "Epoch 487/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9781\n",
            "Epoch 487: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0712 - accuracy: 0.9781 - val_loss: 0.1825 - val_accuracy: 0.9374\n",
            "Epoch 488/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9818\n",
            "Epoch 488: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0675 - accuracy: 0.9818 - val_loss: 0.2119 - val_accuracy: 0.9264\n",
            "Epoch 489/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9806\n",
            "Epoch 489: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.0616 - accuracy: 0.9806 - val_loss: 0.2021 - val_accuracy: 0.9344\n",
            "Epoch 490/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9798\n",
            "Epoch 490: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0679 - accuracy: 0.9798 - val_loss: 0.2271 - val_accuracy: 0.9175\n",
            "Epoch 491/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9766\n",
            "Epoch 491: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0741 - accuracy: 0.9766 - val_loss: 0.2068 - val_accuracy: 0.9264\n",
            "Epoch 492/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9739\n",
            "Epoch 492: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 116ms/step - loss: 0.0755 - accuracy: 0.9739 - val_loss: 0.2091 - val_accuracy: 0.9235\n",
            "Epoch 493/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9798\n",
            "Epoch 493: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0655 - accuracy: 0.9798 - val_loss: 0.1864 - val_accuracy: 0.9314\n",
            "Epoch 494/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9808\n",
            "Epoch 494: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0653 - accuracy: 0.9808 - val_loss: 0.1854 - val_accuracy: 0.9364\n",
            "Epoch 495/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9806\n",
            "Epoch 495: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0708 - accuracy: 0.9806 - val_loss: 0.2211 - val_accuracy: 0.9225\n",
            "Epoch 496/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9808\n",
            "Epoch 496: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0649 - accuracy: 0.9808 - val_loss: 0.2081 - val_accuracy: 0.9245\n",
            "Epoch 497/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9751\n",
            "Epoch 497: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0703 - accuracy: 0.9751 - val_loss: 0.2002 - val_accuracy: 0.9334\n",
            "Epoch 498/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9813\n",
            "Epoch 498: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0639 - accuracy: 0.9813 - val_loss: 0.1937 - val_accuracy: 0.9314\n",
            "Epoch 499/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9806\n",
            "Epoch 499: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 115ms/step - loss: 0.0648 - accuracy: 0.9806 - val_loss: 0.2065 - val_accuracy: 0.9274\n",
            "Epoch 500/500\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9806\n",
            "Epoch 500: val_accuracy did not improve from 0.93936\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 0.0653 - accuracy: 0.9806 - val_loss: 0.1954 - val_accuracy: 0.9324\n"
          ]
        }
      ],
      "source": [
        "#history = model.fit([Pair_train[:, 0], Pair_train[:, 1]], Label_train[:],validation_data=([Pair_test[:, 0], Pair_test[:, 1]], Label_test[:]),batch_size=BATCH_SIZE,epochs=EPOCHS,callbacks=[checkpoint])"
      ],
      "id": "6ac183c6"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kKdXt3SV86aT"
      },
      "outputs": [],
      "source": [
        "#Loading the model\n",
        "model.load_weights(\"/content/weights-333-0.9394.hdf5\")"
      ],
      "id": "kKdXt3SV86aT"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4WWjHnJgsf6",
        "outputId": "613adb26-7c2b-434e-8f9c-261535190da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 11ms/step\n",
            "[[1]\n",
            " [0]\n",
            " [1]\n",
            " ...\n",
            " [0]\n",
            " [1]\n",
            " [0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.99      0.90      0.94       503\n",
            "    Positive       0.91      0.99      0.95       503\n",
            "\n",
            "    accuracy                           0.95      1006\n",
            "   macro avg       0.95      0.95      0.95      1006\n",
            "weighted avg       0.95      0.95      0.95      1006\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# predict on test data\n",
        "y_pred = model.predict([Pair_test[:, 0], Pair_test[:, 1]])\n",
        "\n",
        "# Convert the probabilities to binary predictions using a threshold\n",
        "threshold = 0.5\n",
        "y_pred = np.where(y_pred > threshold, 1, 0)\n",
        "print(y_pred)\n",
        "# Generate a classification report\n",
        "target_names = ['Negative', 'Positive']\n",
        "print(classification_report(Label_test, y_pred, target_names=target_names))"
      ],
      "id": "r4WWjHnJgsf6"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C_YQNPjqJf4",
        "outputId": "7c365d33-c657-4ace-cc77-6f4f0d198bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 9ms/step\n",
            "Accuracy: 0.9473161033797217\n",
            "Precision: 0.9120879120879121\n",
            "Recall: 0.9900596421471173\n",
            "F1 score: 0.9494756911344138\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
        "\n",
        "# predict on test data\n",
        "y_pred = model.predict([Pair_test[:, 0], Pair_test[:, 1]]) #produces similarity score- 0.98 indicated 98% the two audios are similar.\n",
        "\n",
        "# Convert the probabilities to binary predictions using a threshold\n",
        "threshold = 0.5 #because the similarity score ranges between 0 and 1, and 0.5 is the midpoint of this range\n",
        "y_pred = np.where(y_pred > threshold, 1, 0)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(Label_test, y_pred)\n",
        "precision = precision_score(Label_test, y_pred)\n",
        "recall = recall_score(Label_test, y_pred)\n",
        "f1 = f1_score(Label_test, y_pred)\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 score:', f1)\n"
      ],
      "id": "5C_YQNPjqJf4"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "RsOiNAHazUrT",
        "outputId": "52c32bdd-4795-4bab-f95b-46245051287b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 11ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# predict on test data\n",
        "y_pred_prob = model.predict([Pair_test[:, 0], Pair_test[:, 1]])\n",
        "\n",
        "# calculate fpr, tpr, and thresholds\n",
        "fpr, tpr, thresholds = roc_curve(Label_test, y_pred_prob)\n",
        "\n",
        "# calculate AUC\n",
        "roc_auc = roc_auc_score(Label_test, y_pred_prob)\n",
        "\n",
        "# plot ROC curve\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.plot(fpr, tpr, label='Testing set (AUC = %0.2f)' % roc_auc)\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "id": "RsOiNAHazUrT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EKf2eWxbfuR",
        "outputId": "ca4eb92e-ab9a-48ec-fc66-8d9ba7479e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.9/dist-packages (1.1.11)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (2.27.1)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (3.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug"
      ],
      "id": "3EKf2eWxbfuR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49sROPts9hYc"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import math\n",
        "import nlpaug.augmenter.audio as naa\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Lambda\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "49sROPts9hYc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2FF-D_O86T9"
      },
      "outputs": [],
      "source": [
        "## METAMORPHIC TESTING\n",
        "#Creating a function for augmenting the data with noise.\n",
        " \n",
        "def RMS(audio_signal):\n",
        "    RMS=math.sqrt(np.mean(audio_signal**2))\n",
        "    return RMS\n",
        "\n",
        "def RMS_noise_required(audio_signal,required_SNR):\n",
        "    RMS_n=(RMS(audio_signal)/(10**required_SNR))\n",
        "    return RMS_n    \n",
        "\n",
        "def Audio_Augmenter(Noise_Loc,Required_SNR):\n",
        "    # Defining the functiom to calculate RMS and required RMS value of noise as \n",
        "    # per Signal to Noise Ratio(SNR).\n",
        "    \n",
        "    # Adding traffic noise as background noise.\n",
        "\n",
        "    noise, sample_rate1 = librosa.load(Noise_Loc)\n",
        "\n",
        "    #Setting SNR of the noise as required\n",
        "\n",
        "    noise_snr=noise*(RMS_noise_required(noise,Required_SNR)/RMS(noise))\n",
        "\n",
        "    #Creating object for augmentation\n",
        "    \n",
        "    coverage=np.random.rand(1)\n",
        "\n",
        "    aug = naa.NoiseAug(zone=(0.2,0.8),coverage=coverage,noises=[noise_snr])\n",
        "    \n",
        "    return(aug)\n",
        "\n"
      ],
      "id": "F2FF-D_O86T9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTGsw_8L86VT"
      },
      "outputs": [],
      "source": [
        "#Defining function for metamorphic testing\n",
        "from skimage.transform import resize\n",
        "def metamorphic_testing(MR,Test_Data_Loc,Labels):\n",
        "    \n",
        "    scores=[]\n",
        "    \n",
        "    mr_violations_list=[]\n",
        "    \n",
        "    if MR=='Loudness':\n",
        "        \n",
        "        loudness_list=input('Enter the values of loudness for testing. Values should be a tuple and tuples should be seperated by \";\"')\n",
        "        tuple_list=[]\n",
        "\n",
        "        for i in loudness_list.split(';'):\n",
        "            num_list=[]\n",
        "            num_list.append(float(i[1]))\n",
        "            num_list.append(float(i[3]))\n",
        "            tuple_list.append(tuple(num_list))\n",
        "        Factor_List=tuple_list\n",
        "        \n",
        "    if MR=='Noise':\n",
        "            \n",
        "        snr_str_list=input('Enter the values of SNR for testing')\n",
        "            \n",
        "        snr_list=[]\n",
        "            \n",
        "        snr_values=snr_str_list.split(',')\n",
        "            \n",
        "        for snr_value in snr_values:\n",
        "            snr_list.append(float(snr_value))\n",
        "            \n",
        "        Factor_List=snr_list\n",
        "            \n",
        "        Noise_Loc=input(\"Enter the file location of the noise\")\n",
        "        \n",
        "    if MR=='Pitch':\n",
        "        \n",
        "        pitch_list=input('Enter the values of pitch for testing. Values should be a tuple and tuples should be seperated by \";\"')\n",
        "        tuple_list=[]\n",
        "\n",
        "        for i in pitch_list.split(';'):\n",
        "            num_list=[]\n",
        "            num_list.append(float(i[1]))\n",
        "            num_list.append(float(i[3]))\n",
        "            tuple_list.append(tuple(num_list))\n",
        "        Factor_List=tuple_list\n",
        "\n",
        "    if MR=='trim':\n",
        "        top_db = input('Enter the values of SNR for testing')\n",
        "        db_list=[]\n",
        "            \n",
        "        db_values=top_db.split(',')\n",
        "            \n",
        "        for db_value in db_values:\n",
        "            db_list.append(float(db_value))\n",
        "            \n",
        "        Factor_List=db_list\n",
        "    \n",
        "    if MR=='time stretch':\n",
        "        rate = input('Enter the values of SNR for testing')\n",
        "        rate_list=[]\n",
        "            \n",
        "        rate_values=rate.split(',')\n",
        "            \n",
        "        for rate_value in rate_values:\n",
        "            rate_list.append(float(rate_value))\n",
        "            \n",
        "        Factor_List=rate_list\n",
        "\n",
        "    \n",
        "    #Getting mel spectograms for all the audio clips in the original test data and evaluating it.\n",
        "    \n",
        "    X_test=Test_Data_Loc\n",
        "\n",
        "    X_test_org=[]\n",
        "\n",
        "    for audio in X_test:\n",
        "        signal, sample_rate = librosa.load(audio, sr=22050,duration=3)\n",
        "        hop_length=512\n",
        "        mel_spectogram=librosa.feature.melspectrogram(y=signal,sr=sample_rate,hop_length=hop_length)\n",
        "        S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "        S_dB_resized = resize(S_dB, (128, 130), anti_aliasing=True)\n",
        "        X_test_org.append(S_dB_resized)\n",
        "    X_test_org=np.array(X_test_org)\n",
        "\n",
        "    # add a channel dimension to the images\n",
        "\n",
        "    X_test_org=np.expand_dims(X_test_org, axis=-1)\n",
        "\n",
        "    X_test_org=X_test_org/255.0\n",
        "    \n",
        "    #Making Data Pair for Clean Test Data \n",
        "    Pair_test_org,Label_test_org=make_pairs(X_test_org,Labels)\n",
        "    \n",
        "    \n",
        "    acc=model.evaluate([Pair_test_org[:,0],Pair_test_org[:,1]],Label_test_org[:])[1]\n",
        "    \n",
        "    scores.append(acc)\n",
        "    \n",
        "    #Getting the indicies of the correctly classified points\n",
        "    \n",
        "    acc_org=model.predict([Pair_test_org[:,0],Pair_test_org[:,1]])\n",
        "    \n",
        "    y_pred_org=[]\n",
        "    \n",
        "    for i in range(acc_org.shape[0]):\n",
        "        if acc_org[i]>0.5:\n",
        "            y_pred_org.append(1)\n",
        "        else:\n",
        "            y_pred_org.append(0)\n",
        "        \n",
        "    correct_index=[]\n",
        "    \n",
        "    for i in range(len(y_pred_org)):\n",
        "        if y_pred_org[i]==Label_test_org[i]:\n",
        "            correct_index.append(i)\n",
        "   \n",
        "    \n",
        "    #Evaluating the augmented data \n",
        " \n",
        "    for i in range(len(Factor_List)):\n",
        "    \n",
        "    \n",
        "    \n",
        "        #Getting mel spec for the augmented audioclips with traffic noise.SNR=0\n",
        "\n",
        "    \n",
        "        Factor=Factor_List[i]\n",
        "        \n",
        "        if MR=='Loudness':\n",
        "\n",
        "            aug= naa.LoudnessAug(zone=(0.2, 0.8), coverage=1.0, factor=Factor)\n",
        "            \n",
        "        if MR=='Noise':\n",
        "            \n",
        "            aug=Audio_Augmenter(Noise_Loc,Factor)\n",
        "            \n",
        "        if MR=='Pitch':\n",
        "            \n",
        "            aug = naa.PitchAug(sampling_rate=sample_rate,factor=Factor)\n",
        "            \n",
        "            \n",
        "        X_test_aug=[]\n",
        "\n",
        "        index=0\n",
        "        index_aug=0\n",
        "        for audio in X_test:\n",
        "            index=index+1\n",
        "            p=np.random.rand(1)\n",
        "            if p<0.2:\n",
        "                if MR=='trim':\n",
        "                   index_aug=index_aug+1\n",
        "                   signal, sample_rate = librosa.load(audio, sr=22050,duration=3)\n",
        "                   augmented_data = librosa.effects.trim(y=signal,top_db=Factor)\n",
        "                   hop_length=512\n",
        "                   mel_spectogram=librosa.feature.melspectrogram(y=augmented_data[0],sr=sample_rate,hop_length=hop_length)\n",
        "                   S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "                   S_dB_resized = resize(S_dB, (128, 130), anti_aliasing=True)\n",
        "                   X_test_aug.append(S_dB_resized)\n",
        "\n",
        "                elif MR=='time stretch':\n",
        "                   index_aug=index_aug+1\n",
        "                   signal, sample_rate = librosa.load(audio, sr=22050,duration=3)\n",
        "                   #signal_np = np.asarray(signal)\n",
        "                   augmented_data = librosa.effects.time_stretch(y=signal, rate=Factor) \n",
        "                   hop_length=512\n",
        "                   mel_spectogram=librosa.feature.melspectrogram(y=augmented_data,sr=sample_rate,hop_length=hop_length)\n",
        "                   S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "                   S_dB_resized = resize(S_dB, (128, 130), anti_aliasing=True)\n",
        "                   X_test_aug.append(S_dB_resized)\n",
        "    \n",
        "                elif MR=='Loudness' or MR=='Pitch' or MR=='Noise':\n",
        "              \n",
        "                   index_aug=index_aug+1\n",
        "                   signal, sample_rate = librosa.load(audio, sr=22050,duration=3)\n",
        "                   #signal_np = np.asarray(signal)\n",
        "                   augmented_data = aug.augment(signal)\n",
        "                   hop_length=512\n",
        "                   mel_spectogram=librosa.feature.melspectrogram(y=augmented_data[0],sr=sample_rate,hop_length=hop_length)\n",
        "                   S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "                   S_dB_resized = resize(S_dB, (128, 130), anti_aliasing=True)\n",
        "                   X_test_aug.append(S_dB_resized)\n",
        "            else:\n",
        "                signal, sample_rate = librosa.load(audio, sr=22050,duration=3)\n",
        "                hop_length=512\n",
        "                mel_spectogram=librosa.feature.melspectrogram(y=signal,sr=sample_rate,hop_length=hop_length)\n",
        "                S_dB = librosa.power_to_db(mel_spectogram, ref=np.max)\n",
        "                S_dB_resized = resize(S_dB, (128, 130), anti_aliasing=True)\n",
        "                X_test_aug.append(S_dB_resized)\n",
        "\n",
        "        X_test_aug=np.array(X_test_aug)  \n",
        "\n",
        "        # add a channel dimension to the images\n",
        "\n",
        "        X_test_aug=np.expand_dims(X_test_aug, axis=-1)\n",
        "\n",
        "        X_test_aug=X_test_aug/255\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "        #Making Data Pair for Augmented Test Data\n",
        "        Pair_test_aug,Label_test_aug=make_pairs(X_test_aug,Labels)\n",
        "\n",
        "        acc=model.evaluate([Pair_test_aug[:,0],Pair_test_aug[:,1]],Label_test_aug[:])[1]\n",
        "        \n",
        "        scores.append(acc)\n",
        "        \n",
        "        acc1=model.predict([Pair_test_aug[:,0],Pair_test_aug[:,1]])\n",
        "        \n",
        "        y_pred=[]\n",
        "\n",
        "        for i in range(acc1.shape[0]):\n",
        "            if acc1[i]>0.5:\n",
        "                y_pred.append(1)\n",
        "            else:\n",
        "                y_pred.append(0)\n",
        "        \n",
        "        \n",
        "        false=[]\n",
        "        for i in range(len(correct_index)):\n",
        "            if y_pred[correct_index[i]]!=Label_test_aug[correct_index[i]]:\n",
        "                false.append(correct_index[i])\n",
        "        \n",
        "        mr_violations=(len(false)/len(correct_index))*100\n",
        "        \n",
        "        mr_violations_list.append(mr_violations)\n",
        "        \n",
        "        print('mr_violation:',mr_violations)\n",
        "    \n",
        "    print('Acuuracy when evaluated on the clean test data:',scores[0])\n",
        "    print('Acuuracy when evaluated on the augmented test data:',scores[1:])\n",
        "    \n",
        "    print(\"MR Violations for repective \"+ MR +\" Factors:\",mr_violations_list)\n",
        "    \n",
        "    return scores,mr_violations_list"
      ],
      "id": "hTGsw_8L86VT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGnc7xDZ86i_",
        "outputId": "f03315d3-c369-4335-b561-eff8d6444f3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the values of loudness for testing. Values should be a tuple and tuples should be seperated by \";\"(0.2,0.3);(0.5,0.7);(1,2)\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1473 - accuracy: 0.9483\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1575 - accuracy: 0.9443\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 5.136268343815513\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1596 - accuracy: 0.9453\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 4.926624737945493\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1261 - accuracy: 0.9573\n",
            "32/32 [==============================] - 0s 10ms/step\n",
            "mr_violation: 3.7735849056603774\n",
            "Acuuracy when evaluated on the clean test data: 0.948310136795044\n",
            "Acuuracy when evaluated on the augmented test data: [0.9443339705467224, 0.9453280568122864, 0.9572564363479614]\n",
            "MR Violations for repective Loudness Factors: [5.136268343815513, 4.926624737945493, 3.7735849056603774]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.948310136795044,\n",
              "  0.9443339705467224,\n",
              "  0.9453280568122864,\n",
              "  0.9572564363479614],\n",
              " [5.136268343815513, 4.926624737945493, 3.7735849056603774])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metamorphic_testing('Loudness',X_test,y_test)"
      ],
      "id": "MGnc7xDZ86i_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOD3E8vM86op",
        "outputId": "24463dda-ca17-418e-ed02-25960055c141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the values of pitch for testing. Values should be a tuple and tuples should be seperated by \";\"(0.3,0.4);(0.5,0.8);(1,1.5);(1.5,2)\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1393 - accuracy: 0.9414\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.1859 - accuracy: 0.9334\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "mr_violation: 6.335797254487856\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2429 - accuracy: 0.9155\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "mr_violation: 7.708553326293559\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1883 - accuracy: 0.9344\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 6.335797254487856\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2045 - accuracy: 0.9245\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 7.286166842661035\n",
            "Acuuracy when evaluated on the clean test data: 0.9413518905639648\n",
            "Acuuracy when evaluated on the augmented test data: [0.9333996176719666, 0.9155069589614868, 0.9343936443328857, 0.9244532585144043]\n",
            "MR Violations for repective Pitch Factors: [6.335797254487856, 7.708553326293559, 6.335797254487856, 7.286166842661035]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.9413518905639648,\n",
              "  0.9333996176719666,\n",
              "  0.9155069589614868,\n",
              "  0.9343936443328857,\n",
              "  0.9244532585144043],\n",
              " [6.335797254487856, 7.708553326293559, 6.335797254487856, 7.286166842661035])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metamorphic_testing('Pitch',X_test,y_test)"
      ],
      "id": "vOD3E8vM86op"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCdsTwFG86p3",
        "outputId": "8cd969e7-a832-48f5-f51c-a89113d1d657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the values of SNR for testing20,30,40,50\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1519 - accuracy: 0.9443\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1469 - accuracy: 0.9404\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 5.684210526315789\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1668 - accuracy: 0.9394\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 5.36842105263158\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1468 - accuracy: 0.9404\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 5.684210526315789\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1858 - accuracy: 0.9364\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 5.684210526315789\n",
            "Acuuracy when evaluated on the clean test data: 0.9443339705467224\n",
            "Acuuracy when evaluated on the augmented test data: [0.9403578639030457, 0.9393638372421265, 0.9403578639030457, 0.9363816976547241]\n",
            "MR Violations for repective trim Factors: [5.684210526315789, 5.36842105263158, 5.684210526315789, 5.684210526315789]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.9443339705467224,\n",
              "  0.9403578639030457,\n",
              "  0.9393638372421265,\n",
              "  0.9403578639030457,\n",
              "  0.9363816976547241],\n",
              " [5.684210526315789, 5.36842105263158, 5.684210526315789, 5.684210526315789])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metamorphic_testing('trim',X_test,y_test)"
      ],
      "id": "RCdsTwFG86p3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Og3PYYj86tn",
        "outputId": "52b95bbb-6298-43c8-9ed2-61439a062731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the values of SNR for testing2,3,4,5\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1376 - accuracy: 0.9483\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6862 - accuracy: 0.8360\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 16.247379454926623\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7618 - accuracy: 0.8360\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "mr_violation: 16.247379454926623\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.7946 - accuracy: 0.8320\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "mr_violation: 16.771488469601678\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7902 - accuracy: 0.8151\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 18.763102725366878\n",
            "Acuuracy when evaluated on the clean test data: 0.948310136795044\n",
            "Acuuracy when evaluated on the augmented test data: [0.8359841108322144, 0.8359841108322144, 0.8320079445838928, 0.815109372138977]\n",
            "MR Violations for repective time stretch Factors: [16.247379454926623, 16.247379454926623, 16.771488469601678, 18.763102725366878]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.948310136795044,\n",
              "  0.8359841108322144,\n",
              "  0.8359841108322144,\n",
              "  0.8320079445838928,\n",
              "  0.815109372138977],\n",
              " [16.247379454926623,\n",
              "  16.247379454926623,\n",
              "  16.771488469601678,\n",
              "  18.763102725366878])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metamorphic_testing('time stretch',X_test,y_test)"
      ],
      "id": "5Og3PYYj86tn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvMXTj_T86u6",
        "outputId": "60e3eafa-3c64-4744-bf35-c9401cf2dec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the values of SNR for testing-1,2,3,-2\n",
            "Enter the file location of the noise/content/traffic-sound-111442.mp3\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1575 - accuracy: 0.9453\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4653 - accuracy: 0.8390\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 15.983175604626709\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1602 - accuracy: 0.9423\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 5.888538380651945\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1584 - accuracy: 0.9443\n",
            "32/32 [==============================] - 0s 9ms/step\n",
            "mr_violation: 5.047318611987381\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9883 - accuracy: 0.7913\n",
            "32/32 [==============================] - 0s 8ms/step\n",
            "mr_violation: 21.03049421661409\n",
            "Acuuracy when evaluated on the clean test data: 0.9453280568122864\n",
            "Acuuracy when evaluated on the augmented test data: [0.8389661908149719, 0.942345917224884, 0.9443339705467224, 0.7912524938583374]\n",
            "MR Violations for repective Noise Factors: [15.983175604626709, 5.888538380651945, 5.047318611987381, 21.03049421661409]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.9453280568122864,\n",
              "  0.8389661908149719,\n",
              "  0.942345917224884,\n",
              "  0.9443339705467224,\n",
              "  0.7912524938583374],\n",
              " [15.983175604626709, 5.888538380651945, 5.047318611987381, 21.03049421661409])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metamorphic_testing('Noise',X_test,y_test)"
      ],
      "id": "bvMXTj_T86u6"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hM_fkKqrXowF"
      },
      "id": "hM_fkKqrXowF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}